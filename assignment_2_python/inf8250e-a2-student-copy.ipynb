{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b624da4",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 2 (133 pts total)\n",
    "\n",
    "## Instructions:\n",
    "* This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
    "* Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
    "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
    "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
    "* If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
    "\n",
    "This assignment has three parts. Part 1 will focus on the Monte Carlo method, you will learn:\n",
    "\n",
    "1. To use the Monte Carlo method for control\n",
    "2. To use Monte Carlo estimates for off-policy prediction using importance sampling\n",
    "\n",
    "Part 2 will focus on *prediction*. You will learn:\n",
    "\n",
    "1. To use Monte Carlo estimates for prediction\n",
    "2. To use Temporal difference methods for prediction\n",
    "3. To understand the relationship between the two, and unifying the algorithms\n",
    "\n",
    "Part 3 will focus on Temporal Difference control methods. You will learn:\n",
    "\n",
    "1. To use SARSA for optimal control\n",
    "2. To use ExpectedSARSA for optimal control\n",
    "3. To use Q-learning for optimal control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1afcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install otter-grader\n",
    "!git clone https://github.com/chandar-lab/INF8250e-assignments-public.git public\n",
    "# # Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(colab=True, tests_dir='./public/assignment_2_python/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seed\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43cf234",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Consider a variation of the ```FrozenLake-v0``` environment from the OpenAI gym library. The agent controls the movement of a character in a grid world. Some tiles of the frozen and walkable (```F```), while others are holes in the lake (```H```) that lead to the agent falling into the water. The states are denoted similarly to the GridWorld in assignment 1: $S=\\{0,1,2,..., 14, 15\\}$ for a 4x4 lake. The agent starts on a starting tile (```S```), and its goal is to find the fastest walkable path to a goal tile (```G```).\n",
    "\n",
    "The agent can move in the four cardinal directions, $A=\\{left, down, right, up\\}$, but the floor is slippery! Given a `slip_rate` of $0 \\leq \\xi < 1$, the agent will go in a random wrong direction with probability $\\xi$. \n",
    "\n",
    "\n",
    "The reward is $-1$ on all transitions, except for three cases that all result in the episode terminating: (1) The agent falling into a hole nets the agent a reward of $-100$, (2) The agent takes over 100 steps, all the ice melts and the agent gets a reward of $-100$, and (3) The agent reaches the goal state, rewarding it a reward of $0$. The discount factor for this environment should be set to $\\gamma = 0.99$. The environment is implemented for you below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f0acd",
   "metadata": {},
   "source": [
    "Example 4x4 FrozenLake environment\n",
    "\n",
    "|            |            |            |            |\n",
    "|------------|------------|------------|------------|\n",
    "| `S`   | `F` |`F` | `F` |\n",
    "| `F` | `H` | `F` | `H` |\n",
    "| `F` | `F` |`F` | `H` |\n",
    "| `H`| `F` | `F` | `G` |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ac96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "from gym import utils\n",
    "\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"2x2\": [\"SF\", \"HG\"],\n",
    "    \"4x4-easy\":[\"SFFF\", \"FHFF\", \"FFFF\", \"HFFG\"],\n",
    "    \"4x4\": [\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"],\n",
    "    \"6x5\": [\n",
    "        \"SFFFF\",\n",
    "        \"FFFFF\",\n",
    "        \"FHFFF\",\n",
    "        \"FHFFF\",\n",
    "        \"FFFFF\",\n",
    "        \"GFFFF\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def categorical_sample(prob_n, np_random):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.random()).argmax()\n",
    "\n",
    "\n",
    "class DiscreteEnv(Env):\n",
    "\n",
    "    \"\"\"\n",
    "    Has the following members\n",
    "    - nS: number of states\n",
    "    - nA: number of actions\n",
    "    - P: transitions (*)\n",
    "    - isd: initial state distribution (**)\n",
    "    (*) dictionary of lists, where\n",
    "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "    (**) list or array of length nS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nS, nA, P, isd, max_length=50):\n",
    "        self.P = P\n",
    "        self.isd = isd\n",
    "        self.lastaction = None  # for rendering\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "\n",
    "        self.seed()\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        self.t = 0\n",
    "        return int(self.s)\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        if self.t >= self.max_length:\n",
    "            d = True\n",
    "            r = -100\n",
    "        self.t += 1\n",
    "        return (int(s), r, d, {\"prob\": p})\n",
    "\n",
    "class FrozenLakeEnv(DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"ansi\"]}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\", slip_rate=0.5):\n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc, dtype=\"c\")\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b\"S\").astype(\"float64\").ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row * ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            newrow, newcol = inc(row, col, action)\n",
    "            newstate = to_s(newrow, newcol)\n",
    "            newletter = desc[newrow, newcol]\n",
    "            done = bytes(newletter) in b\"GH\"\n",
    "            # reward = float(newletter == b\"G\")\n",
    "            reward = -1\n",
    "            # if newletter == b\"H\":\n",
    "            #     reward = -100\n",
    "            done = False\n",
    "            return newstate, reward, done\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter == b\"G\":\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    if letter == b'H':\n",
    "                        li.append((1.0, s, -100, True))\n",
    "                    else:\n",
    "                        if slip_rate > 0:\n",
    "                            li.append((slip_rate/2.0, *update_probability_matrix(row, col, (a - 1) % 4)))\n",
    "                            li.append((1 - slip_rate, *update_probability_matrix(row, col, a)))\n",
    "                            li.append((slip_rate/2.0, *update_probability_matrix(row, col, (a + 1) % 4)))\n",
    "                        else:\n",
    "                            li.append((1.0, *update_probability_matrix(row, col, a)))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        outfile = StringIO() if mode == \"ansi\" else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\n",
    "                \"  ({})\\n\".format([\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction])\n",
    "            )\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(\"\".join(line) for line in desc) + \"\\n\")\n",
    "\n",
    "        if mode != \"human\":\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113df1d",
   "metadata": {},
   "source": [
    "# Part 0 - Helper Methods (5pts)\n",
    "First, let us define some helper methods that will be useful for the entire assignment. We give here three methods that may use or not use at any point of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77609d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    \"\"\"\n",
    "    Input: state (int) [0, .., 15]\n",
    "    output: action (int) [0,1,2,3]\n",
    "    \"\"\"\n",
    "    return np.random.randint(0,4)\n",
    "\n",
    "def plot_many(experiments, label=None, color=None):\n",
    "    mean_exp = np.mean(experiments, axis=0)\n",
    "    std_exp = np.std(experiments, axis=0)\n",
    "    plt.plot(mean_exp, color=color, label=label)\n",
    "    plt.fill_between(range(len(experiments[0])), mean_exp + std_exp, mean_exp - std_exp, color=color, alpha=0.1)\n",
    "    \n",
    "def random_argmax(value_list):\n",
    "    \"\"\" a random tie-breaking argmax \"\"\"\n",
    "    values = np.asarray(value_list)\n",
    "    return np.argmax(np.random.random(values.shape) * (values==values.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98269f88",
   "metadata": {},
   "source": [
    "## Question 1 - Creating some helper methods (5pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617fb811",
   "metadata": {},
   "source": [
    "### Question 1a) (2 pts)\n",
    "Implement an epsilon-greedy policy over the state-action values of an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402d061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_eps_greedy_policy(state_action_values, epsilon):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        state_action_values (list[list]): first axis maps over states of an environment, and second axis the actions.\n",
    "                                        The stored values are the state-action values corresponding to the state and action index\n",
    "        epsilon (float): Probability of taking a random action\n",
    "    Returns policy (int -> int): method taking a state and returning a sampled action to take\n",
    "    \"\"\"\n",
    "    def policy(state):\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------------\n",
    "        ...\n",
    "        # --------------------------------\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef1f1b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 0.1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a16da4",
   "metadata": {},
   "source": [
    "### Question 1b) (3 pts)\n",
    "**b)** Create a function `generate_episode` which takes as input a policy $\\pi$ (like the one outputted by **question 1a**), the environment, and the boolean `render` which renders every step of the episode in text form (rendering the episode is as easy as calling `env.render()`). The output of this function should return the tuple $\\texttt{states}, \\texttt{actions}, \\texttt{rewards}$ containing the states, actions, and rewards of the generated episode following $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d00db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_episode(policy, env, render=False): \n",
    "    \"\"\"\n",
    "    Input:\n",
    "        policy (int -> int): policy taking a state as an input and outputs a given action\n",
    "        env (DiscreteEnv): The FrozenLake environment\n",
    "        render (bool): Whether or not to render the episode\n",
    "    Returns:\n",
    "        states (list): the sequence of states in the generated episode\n",
    "        actions (list): the sequence of actions in the generated episode\n",
    "        rewards (list): the sequence of rewards in the generated episode \n",
    "    \"\"\"\n",
    "    states = []\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    done = False\n",
    "    # --------------------------------\n",
    "    ...\n",
    "    # --------------------------------\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5b6ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 0.1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4104f8",
   "metadata": {},
   "source": [
    "# Part 1 - Monte Carlo Methods (27 pts)\n",
    "\n",
    "Consider in this section the 6x6 version of the FrozenLake environment, with a `slip_rate` of 0.1. Again, make sure to use a discount factor of $\\gamma=0.99$ for all your experiments. This environment can be instantiated with `{env = FrozenLakeEnv(map_name=\"6x6\", slip_rate=0.1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959aed4",
   "metadata": {},
   "source": [
    "## Question 1 (15 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366dfdd",
   "metadata": {},
   "source": [
    "### Question 1a) (5pts)\n",
    "Implement the first-visit Monte Carlo (for $\\epsilon$-soft policies) control algorithm to find the approximate optimal policy $\\pi \\approx \\pi_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0794787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fv_mc_estimation(states, actions, rewards, discount):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        states (list): states of an episode generated from generate_episode\n",
    "        actions (list): actions of an episode generated from generate_episode\n",
    "        rewards (list): rewards of an episode generated from generate_episode\n",
    "        discount (float): discount factor\n",
    "    Returns visited_states_returns (dictionary): \n",
    "        keys are all the unique state-action combinations in the episode\n",
    "        values are the estimated discounted return of the first visited pair\n",
    "    \"\"\"\n",
    "    visited_states_returns = {}\n",
    "    # TO IMPLEMENT\n",
    "    # --------------------------------\n",
    "    ...\n",
    "    # --------------------------------\n",
    "    return visited_states_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabfcec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1.1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b438c6a",
   "metadata": {},
   "source": [
    "Given your implementation of `fv_mc_estimation`, we can now do control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd56b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fv_mc_control(env, epsilon=0.05, num_episodes=100, discount=0.99):\n",
    "    # Initialize memory of estimated state-action returns\n",
    "    state_action_returns = [[[] for j in range(env.action_space.n)] for i in range(env.observation_space.n)]\n",
    "    all_returns = []\n",
    "    \n",
    "    for j in range(num_episodes):\n",
    "        state_action_values = [[np.mean(a) for a in s] for s in state_action_returns]\n",
    "        policy = make_eps_greedy_policy(state_action_values, epsilon)\n",
    "        states, actions, rewards = generate_episode(policy, env)\n",
    "        visited_states_returns = fv_mc_estimation(states, actions, rewards, discount)\n",
    "        for sa in visited_states_returns:\n",
    "            s, a = sa\n",
    "            state_action_returns[s][a].append(visited_states_returns[sa])\n",
    "        all_returns.append(np.sum(rewards))\n",
    "    \n",
    "    state_action_values = [[np.mean(a) for a in s] for s in state_action_returns]\n",
    "    return state_action_values, all_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bd680",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1b) - Plotting (3pts)\n",
    "\n",
    "Let $\\epsilon=0.05$, run the algorithm for 2000 episodes, and repeat this experiment for 5 different runs. Plot the average undiscounted return across the 5 different runs with respect to the number of episodes (x-axis is the 1000 episodes, y-axis is the return for each episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "all_sa_values, all_returns = [], []\n",
    "for i in range(5):\n",
    "    sa_values, returns = fv_mc_control(env, epsilon=0.05, num_episodes=2000)\n",
    "    all_sa_values.append(sa_values)\n",
    "    all_returns.append(returns)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "plot_many(all_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aca36b",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1c) (2 pts)\n",
    "Visualize an episode during evaluation with the last learned state-action value tables using the code below. For clarity, let's evaluate an episode with `0 slip_rate` and $\\epsilon=0$. In the absence of a slip-rate and exploration, what is the return of the optimal policy for all 5 learned state-action value tables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef45a9e",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8235d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize path\n",
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "optimal_policy = make_eps_greedy_policy(all_sa_values[-1], epsilon=0.)\n",
    "\n",
    "s, a, r = generate_episode(optimal_policy, env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c17481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the return during evaluation\n",
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "for i in range(5):\n",
    "    optimal_policy = make_eps_greedy_policy(all_sa_values[i], epsilon=0.)\n",
    "    s, a, r = generate_episode(optimal_policy, env, render=False)\n",
    "    print('Return is ' + str(np.sum(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0d78c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1d) - Plotting again (2pts)\n",
    "##### Now repeat the exercise from b), but set $\\epsilon=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004478bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "\n",
    "# Set seed\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "all_sa_values_c, all_returns_c = [], []\n",
    "for i in range(5):\n",
    "    sa_values, returns = fv_mc_control(env, epsilon=0.5, num_episodes=2000)\n",
    "    all_sa_values_c.append(sa_values)\n",
    "    all_returns_c.append(returns)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "plot_many(all_returns_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89802b25",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1e)  (3pts)\n",
    " Again, evaluate the policies derived from the learned sate-action value tables under 0 `slip_rate` and $\\epsilon=0.$\n",
    " You should notice now that some of your learned policies produce returns that are **lower** than the previous optimal return found. Visualize the path taken from one of these \"sub-optimal\" policies. Why does this happen? Answer in 1 to 3 sentences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313028b",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize path taken\n",
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "optimal_policy = make_eps_greedy_policy(all_sa_values_c[-1], epsilon=0.)\n",
    "s, a, r = generate_episode(optimal_policy, env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed253ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation return \n",
    "for i in range(5):\n",
    "    env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "    optimal_policy = make_eps_greedy_policy(all_sa_values_c[i], epsilon=0.)\n",
    "    s, a, r = generate_episode(optimal_policy, env, render=False)\n",
    "    print('Return is ' + str(np.sum(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e835bda",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2 (12 pts)\n",
    "Consider a random behavior policy $b(a|s) = 0.25$ for every $a$ and $s$. In this section, we will use episodes generated with $b$ to evaluate the value function of $V^\\pi(s)$ for some arbitrary $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061aa6ce",
   "metadata": {},
   "source": [
    "### Question 2a) (2 pts)\n",
    " Before implementing the importance sampling algorithms, let's write a function that returns the epsilon greedy policy with respect to state action values again. But this time, let it return the probability distribution over actions instead of a sampled action. $\\pi(a|s): \\mathcal{S} \\rightarrow \\mathbb{R}^{|\\mathcal{A}|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42802be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_eps_greedy_policy_distribution(state_action_values, epsilon):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        state_action_values (list[list]): first axis maps over states of an environment, and second axis the actions.\n",
    "                                        The stored values are the state-action values corresponding to the state and action index\n",
    "        epsilon (float): Probability of taking a random action\n",
    "    Returns policy (int -> list[float]): method taking a state and returning the \n",
    "                                        probability distribution over actions\n",
    "    \"\"\"\n",
    "    def policy(state):\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------------\n",
    "        ...\n",
    "        # --------------------------------\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268897d9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1.2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1907c",
   "metadata": {},
   "source": [
    "### Question 2b) (5 pts)\n",
    "Implement the `is_mc_estimate_with_ratios` method that computes all importance sampling estimates of the states of an episode of a given target policy given the random behavior policy. Let it also return all the associated importance sampling ratios. **DO NOT USE AN INCREMENTAL IMPLEMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf41dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_mc_estimate_with_ratios(states, actions, rewards, target_policy, discount):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        states (list): states of an episode generated from generate_episode\n",
    "        actions (list): actions of an episode generated from generate_episode\n",
    "        rewards (list): rewards of an episode generated from generate_episode\n",
    "        target_policy (int -> list[float]): The target policy that takes in a state and returns \n",
    "                                            an action probability distrtibution\n",
    "        discount (float): discount factor\n",
    "    Returns states_returns_and_ratios (dictionary): \n",
    "        Keys are all the states visited in the input episode\n",
    "        Values is a list of tuples. The first index of the tuple is the IS estimate of the discounted returns\n",
    "            of that state in the episode. The second index is the IS ratio associated with each of the IS estimates. \n",
    "                i.e: if state 2 is visited 3 times in the episode, \n",
    "                state_returns_and_ratios[2] should be a list of 3 tuples.\n",
    "    \"\"\"\n",
    "    state_returns_and_ratios = {}\n",
    "    # TO IMPLEMENT\n",
    "    # --------------------------------\n",
    "    ...\n",
    "    # --------------------------------\n",
    "    return state_returns_and_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2147696",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1.2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef18b93",
   "metadata": {},
   "source": [
    "Given the off-policy importance sampling estimates, we can now perform Ordinary and weighted importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ois_mc_pred(env, b_policy, target_policy, num_episodes, discount=0.99):\n",
    "    state_returns = [[] for i in range(env.observation_space.n)]\n",
    "    value_function_trace = []\n",
    "    \n",
    "    for j in (range(num_episodes)):\n",
    "        states, actions, rewards = generate_episode(b_policy, env)\n",
    "        sra = is_mc_estimate_with_ratios(states, actions, rewards, target_policy, discount)\n",
    "        for s in sra:\n",
    "            for ra in sra[s]:\n",
    "                state_returns[s].append(ra[0])\n",
    "        value_function_trace.append([np.mean(s) for s in state_returns])\n",
    "    return value_function_trace\n",
    "\n",
    "def wis_mc_pred(env, b_policy, target_policy, num_episodes, discount=0.99):\n",
    "    state_returns = [[] for i in range(env.observation_space.n)]\n",
    "    all_ratios = [[] for i in range(env.observation_space.n)]\n",
    "    value_function_trace = []\n",
    "    \n",
    "    for j in (range(num_episodes)):\n",
    "        states, actions, rewards = generate_episode(b_policy, env)\n",
    "        sra = is_mc_estimate_with_ratios(states, actions, rewards, target_policy, discount)\n",
    "        for s in sra:\n",
    "            for ra in sra[s]:\n",
    "                state_returns[s].append(ra[0])\n",
    "                all_ratios[s].append(ra[1])\n",
    "        value_function_trace.append([np.sum(s)/np.sum(all_ratios[i]) for i,s in enumerate(state_returns)])\n",
    "    return value_function_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccfe556",
   "metadata": {},
   "source": [
    "Now we use *ordinary importance sampling* Monte Carlo prediction and *weighted importance sampling* Monte Carlo prediction to evaluate the $V^\\pi(s)$ for one of the epsilon-greedy policies learned in **2b)**. In both cases, the algorithm is run for 1000 episodes across 3 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa27ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "policy = make_eps_greedy_policy_distribution(all_sa_values[0], 0.05)\n",
    "\n",
    "all_value_function_traces_ois = []\n",
    "for i in range(5):\n",
    "    value_function_trace = ois_mc_pred(env, random_policy, policy, 1000)\n",
    "    all_value_function_traces_ois.append(value_function_trace)\n",
    "\n",
    "all_value_function_traces_wis = []\n",
    "for i in range(5):\n",
    "    value_function_trace = wis_mc_pred(env, random_policy, policy, 1000)\n",
    "    all_value_function_traces_wis.append(value_function_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38bacf8",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2c) - Plotting and Analysis (5pts)\n",
    "Plot the average off policy Monte Carlo estimates of $V^\\pi(s)$ for some specified states against the number of episodes for both *ordinary importance sampling* and *weighted importance sampling*. In theses plots, the standard deviation is included as a confidence band. Which of the two methods do you expect to produce lower variance estimates, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632a83d",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in [11, 12, 16, 17, 18, 20, 25, 27, 28]:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plot_many(np.array(all_value_function_traces_ois)[:,:,state], color='orange', label='ordinary IS')\n",
    "    plot_many(np.array(all_value_function_traces_wis)[:,:,state], color='green', label='Weighted IS')\n",
    "    plt.xlabel('number of episodes')\n",
    "    plt.ylabel('V(s)')\n",
    "    plt.grid()\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e557f9",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part 2 -  Prediction: Unifying Monte Carlo methods and Temporal Difference Learning (54 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cbe35",
   "metadata": {},
   "source": [
    "Consider in this section the same 6x5 FrozenLake environment with a `slip_rate` of 0.1. Use a discount factor of $\\gamma=0.99$. We will be working with the same random policy used above for all questions in this part: $\\pi(a|s) = 0.25$ for all $a$ and $s$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c2161",
   "metadata": {},
   "source": [
    "## Question 1 - MC (10 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d0d39",
   "metadata": {},
   "source": [
    "### Question 1a) (5 pts)\n",
    "Implement the *Every visit Monte Carlo prediction* algorithm in order to estimate $V^\\pi(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20e482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ev_mc_estimate(states, actions, rewards, discount):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        states (list): states of an episode generated from generate_episode\n",
    "        actions (list): actions of an episode generated from generate_episode\n",
    "        rewards (list): rewards of an episode generated from generate_episode\n",
    "        discount (float): discount factor\n",
    "    Returns visited_states_returns (dictionary): \n",
    "        Keys are all the states visited in an the given episode\n",
    "        Values is a list of the estimated MC return of a given state. \n",
    "            i.e: if a state is visited 3 times in an episode, there are 3 estimated returns of that state.\n",
    "    \"\"\"\n",
    "    visited_state_returns = {}\n",
    "    # TO IMPLEMENT \n",
    "    # --------------------------------\n",
    "    ...\n",
    "    # --------------------------------\n",
    "    return visited_state_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d168e0f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2.1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5064de",
   "metadata": {},
   "source": [
    "We now use `ev_mc_estimate` to do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ev_mc_pred(policy, env, num_episodes=100, discount=0.99):\n",
    "    state_returns = [[0] for i in range(env.observation_space.n)]\n",
    "    state_values_trace = []\n",
    "    for j in (range(num_episodes)):\n",
    "        states, actions, rewards = generate_episode(policy, env)\n",
    "        visited_state_returns = ev_mc_estimate(states, actions, rewards, discount)\n",
    "        for s in visited_state_returns:\n",
    "            state_returns[s].extend(visited_state_returns[s])\n",
    "        state_values_trace.append([np.mean(s) for s in state_returns])\n",
    "    return state_values_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da580bf",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1b) - Plotting (5 pts)\n",
    " Train the algorithm for 10000 episodes, and plot the learning curves for each $s$ of $V^\\pi(s)$ over the number of episodes. The result should be 1 figure, with 16 curves plotted inside it (one for each state, x-axis is the 10000 episodes, y-axis is the current estimate of $V^\\pi(s)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "ev_state_vals = ev_mc_pred(random_policy, env, num_episodes=10000, discount=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d3342",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "plt.plot(ev_state_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c82513",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2 - TD(0) (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae18bc",
   "metadata": {},
   "source": [
    "### Question 2a) (5 pts)\n",
    "Implement the `TD(0)` prediction algorithm to estimate $V^\\pi(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883374b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def td0(policy, env, step_size=0.1, num_episodes=100, discount=0.99):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        policy (int -> int): policy to evaluate\n",
    "        env (DiscreteEnv): FrozenLake environment \n",
    "        step_size (float): step size alpha of td learning\n",
    "        num_episodes (int): number of episodes to run the algorithm for\n",
    "        discount (float): discount factor\n",
    "    Returns state_values_trace (list of lists):\n",
    "        Value estimates of each state at every episode of training.\n",
    "    \n",
    "    Do not modify state_values_trace. JUST UPDATE state_values.\n",
    "        state_values keep tracks of the value of each state. Each index of state_values represents one state.\n",
    "    \"\"\"\n",
    "    state_values = [0 for i in range(env.observation_space.n)]\n",
    "    state_values_trace = []\n",
    "    for j in (range(num_episodes)):\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------------\n",
    "        ...\n",
    "        # --------------------------------\n",
    "        state_values_trace.append([s for s in state_values])\n",
    "    return state_values_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d206309",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2.2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ee64e",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2b) - Plotting (5 pts)\n",
    "Use a step size $\\alpha=0.01$. Train the algorithm for 10000 episodes as well, and plot the same figure as in the previous question ($V^\\pi(s)$ for each $s$ over the number of episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "td_state_vals = td0(random_policy, env, step_size=0.01, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e95440",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(td_state_vals)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f929c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3 - TDN (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf04feb",
   "metadata": {},
   "source": [
    "### Question 3a) (5pts)\n",
    "\n",
    "Now, implement the *n-step* `TD` algorithm to estimate $V^\\pi(s)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ef1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tdn(policy, env, n, step_size=0.1, num_episodes=100, discount=0.99):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        policy (int -> int): policy to evaluate\n",
    "        env (DiscreteEnv): FrozenLake environment\n",
    "        n (int): Number of steps before bootstrapping for td(n) algorithm\n",
    "        step_size (float): step size alpha of td learning\n",
    "        num_episodes (int): number of episodes to run the algorithm for\n",
    "        discount (float): discount factor\n",
    "    Returns state_values_trace (list of lists):\n",
    "        Value estimates of each state at every episode of training.\n",
    "    \n",
    "    Do not modify state_values_trace. JUST UPDATE state_values.\n",
    "        state_values keep tracks of the value of each state. Each index of state_values represents one state.\n",
    "    \"\"\"\n",
    "    state_values = [0 for i in range(env.observation_space.n)]\n",
    "    state_values_trace = []\n",
    "    for j in (range(num_episodes)):\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------------\n",
    "        ...\n",
    "        # --------------------------------\n",
    "        state_values_trace.append([s for s in state_values])\n",
    "    return state_values_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398be346",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2.3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b4d67",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3b) - Plotting (2 pts)\n",
    "Use a step size of $\\alpha=0.01$. This algorithm should take the additional hyper-parameter $n$ to determine how much to bootstrap. Now set $n=0$, and train the algorithm for 10000 episodes. Plot the the same figure as before ($V^\\pi(s)$ for each $s$ over the number of episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tdn1_state_vals = tdn(random_policy, env, n=0, step_size=0.01, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6307aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(tdn1_state_vals)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b130c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3c) (3 pts)\n",
    "Compare this figure to `TD(0)` and *Every visit Monte Carlo Prediction*. Which algorithm do you expect this figure to look similar to? Does it, why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15986ba",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c63758",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3d) - Plotting (2 pts)\n",
    "Using the same implementation of *n-step* `TD`, estimate $V^\\pi(s)$ using $n = 100$ instead (still with $\\alpha=0.01$ and 10000 episodes). Again, plot the same figure as before ($V^\\pi(s)$ for each $s$ over the number of episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tdn100_state_vals = tdn(random_policy, env, n=100, step_size=0.01, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43634",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(tdn100_state_vals)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b93229",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3e)  (3 pts)\n",
    "Compare this new figure to `TD(0)` and *Every visit Monte Carlo Prediction*. Which algorithm do you expect this figure to look similar to? Does it, why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f018a9f",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a0130",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 4 - Unifying (9 pts)\n",
    "\n",
    "The intuition is that *n-step* `TD` should generalize both *Monte Carlo prediction* and `TD(0)`. We saw in the previous question that it does not seem to be equivalent to MC prediction. Modify your *n-step* `TD` algorithm such that when $n=100$, it becomes equivalent to *Every visit Monte Carlo prediction*. Hint: This has to do with the step size $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e66b5e",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4a) (3 pts)\n",
    "Before implementing this modified `TDN`, identify what the new formula for $\\alpha$ should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8cc01f",
   "metadata": {},
   "source": [
    "Type your answere here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb88c2",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 4b) (3 pts)\n",
    "Now implement the `modified_tdn` method that uses this new step size. Most of this method is the same as `tdn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362963b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modified_tdn(policy, env, n, num_episodes=100, discount=0.99):\n",
    "    \"\"\"\n",
    "    This function should largely be equivalent to the tdn function implemented in Question 3.\n",
    "    The only difference is the step size will be dynamically changed using your formula in Q5a).\n",
    "    You may copy paste most lines in the previous implementation.\n",
    "    \n",
    "    Input: \n",
    "        policy (int -> int): policy to evaluate\n",
    "        env (DiscreteEnv): FrozenLake environment\n",
    "        n (int): Number of steps before bootstrapping for td(n) algorithm\n",
    "        step_size (float): step size alpha of td learning\n",
    "        num_episodes (int): number of episodes to run the algorithm for\n",
    "        discount (float): discount factor\n",
    "    Returns state_values_trace (list of lists):\n",
    "        Value estimates of each state at every episode of training.\n",
    "    \n",
    "    Do not modify state_values_trace. JUST UPDATE state_values and state_visitation.\n",
    "        state_values keep tracks of the value of each state. Each index of state_values represents one state.\n",
    "    \"\"\"\n",
    "    state_values = [0 for i in range(env.observation_space.n)]\n",
    "    state_visitation = [0 for i in range(env.observation_space.n)]\n",
    "    state_values_trace = []\n",
    "    for j in (range(num_episodes)):\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------------\n",
    "        ...\n",
    "        # --------------------------------\n",
    "        state_values_trace.append([s for s in state_values])\n",
    "    return state_values_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662b428",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2.4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0da54",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4c) - Plotting (3 pts)\n",
    "Now plot the same plot as in the previous questions with $n=100$, and compare it with the *Every Visit MC prediction* algorithm. You should now see that their behaviors match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7df789",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "mod_tdn100_state_vals = modified_tdn(random_policy, env, n=100, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(mod_tdn100_state_vals)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c7fb9",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 5 - Bias-variance tradeoff (10 pts)\n",
    "Finally, we have (at least practically) shown that *n-step* `TD` unifies `TD(0)` and *MC prediction*. Let's further examine how these two algorithms differ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45a335",
   "metadata": {},
   "source": [
    "### Question 5a) (3 pts)\n",
    "Let's implement some metric to evaluate the error of our value estimates. For this question, you can simply compute the mean squared error with the mean value of the last 100 $V^\\pi(s)$ learned for each experiment. If $i$ is the $i$th episode, and $V^\\pi_i(s)$ is the value function at that episode, then `mse`$(V^\\pi_i(s))=\\texttt{mean}((V^\\pi_i(s) - \\text{mean}(V^\\pi_{-100:10000})^2, \\texttt{axis=-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c328c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def value_mse(values):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        values (list[lists]): The output of the tdn method, state_values_trace\n",
    "    Output:\n",
    "        np.array, output of the above function described in Question 6a)\n",
    "    \"\"\"\n",
    "    # TO IMPLEMENT\n",
    "    # --------------------------------\n",
    "    ...\n",
    "    # --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ec955",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2.5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0963190",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5b) - Plotting (2 pts)\n",
    "We will use the original *n-step* `TD` algorithm (with a fixed $\\alpha$) to plot the mean squared errors during learning with $n=1, n=10$, and $n=100$ over 10000 episodes. Do this for $\\alpha=0.5$, $\\alpha=0.1$, and $\\alpha=0.01$. There should be three figures (one for each $\\alpha$), each containing three plots (the three values of $n$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.1)\n",
    "# Set seed\n",
    "seed = 10\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "td0_state_vals_alpha001 = tdn(random_policy, env, n=0, step_size=0.01, num_episodes=10000)\n",
    "td10_state_vals_alpha001 = tdn(random_policy, env, n=10, step_size=0.01, num_episodes=10000)\n",
    "td100_state_vals_alpha001 = tdn(random_policy, env, n=100, step_size=0.01, num_episodes=10000)\n",
    "\n",
    "td0_state_vals_alpha01 = tdn(random_policy, env, n=0, step_size=0.1, num_episodes=10000)\n",
    "td10_state_vals_alpha01 = tdn(random_policy, env, n=10, step_size=0.1, num_episodes=10000)\n",
    "td100_state_vals_alpha01 = tdn(random_policy, env, n=100, step_size=0.1, num_episodes=10000)\n",
    "\n",
    "td0_state_vals_alpha05 = tdn(random_policy, env, n=0, step_size=0.5, num_episodes=10000)\n",
    "td10_state_vals_alpha05 = tdn(random_policy, env, n=10, step_size=0.5, num_episodes=10000)\n",
    "td100_state_vals_alpha05 = tdn(random_policy, env, n=100, step_size=0.5, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "axis[0].plot(value_mse(td0_state_vals_alpha001), label='n=0')\n",
    "axis[0].plot(value_mse(td10_state_vals_alpha001), label='n=10')\n",
    "axis[0].plot(value_mse(td100_state_vals_alpha001), label='n=100')\n",
    "axis[0].grid(alpha=0.5)\n",
    "axis[0].legend()\n",
    "\n",
    "axis[1].plot(value_mse(td0_state_vals_alpha01), label='n=0')\n",
    "axis[1].plot(value_mse(td10_state_vals_alpha01), label='n=10')\n",
    "axis[1].plot(value_mse(td100_state_vals_alpha01), label='n=100')\n",
    "axis[1].grid(alpha=0.5)\n",
    "axis[1].legend()\n",
    "\n",
    "axis[2].plot(value_mse(td0_state_vals_alpha05), label='n=0')\n",
    "axis[2].plot(value_mse(td10_state_vals_alpha05), label='n=10')\n",
    "axis[2].plot(value_mse(td100_state_vals_alpha05), label='n=100')\n",
    "axis[2].grid(alpha=0.5)\n",
    "axis[2].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adaeb8f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    " ### Question 5 c) (5 pts)\n",
    " Using this visualization, explain the bias-variance trade-off of temporal difference methods versus Monte Carlo methods. (Don't worry if the figures for $\\alpha=0.1$ and especially $\\alpha=0.5$ are too noisy, this is just a qualitative inspection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5a1ef",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326763b",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part 3 - Temporal Difference Control Methods (47 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861d04b",
   "metadata": {},
   "source": [
    "Continuing with the same FrozenLake environment as before with 0 `slip_rate` this time, we will investigate various TD-control methods in this section. In this question you need to implement a training procedure similar to the `generate_episode` function in Part 0, but instead of running a fixed policy, you need to ensure that the agent is trained (i.e., value estimate is updated) throughout the learning phase. \n",
    "\n",
    "First, carefully read and understand the code provided for a base class that will serve as the parent class for all learning agents you will implement in this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def agent_init(self, agent_init_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "        {\n",
    "            num_states (int): The number of states,\n",
    "            num_actions (int): The number of actions,\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor,\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(agent_init_info['seed'])\n",
    "        random.seed(agent_init_info['seed'])\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.num_states = agent_init_info[\"num_states\"]\n",
    "        self.epsilon = agent_init_info[\"epsilon\"]\n",
    "        self.step_size = agent_init_info[\"step_size\"]\n",
    "        self.discount = agent_init_info[\"discount\"]\n",
    "        \n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = np.zeros((self.num_states, self.num_actions))\n",
    "    \n",
    "    def get_current_policy(self):\n",
    "        \"\"\"\n",
    "        Returns the epsilon greedy policy of the agent following the previous implementation of \n",
    "        make_eps_greedy_policy\n",
    "        \n",
    "        Returns:\n",
    "            Policy (callable): fun(state) -> action\n",
    "        \"\"\"\n",
    "        return make_eps_greedy_policy(self.q, epsilon=self.epsilon)\n",
    "    \n",
    "    def agent_step(self, prev_state, prev_action, prev_reward, current_state, done):\n",
    "        \"\"\" A learning step for the agent given a state, action, reward, next state and done\n",
    "        Args:\n",
    "            prev_state (int): the state observation from the enviromnents last step\n",
    "            prev_action (int): the action taken given prev_state\n",
    "            prev_reward (float): The reward received for taking prev_action in prev_state\n",
    "            current_state (int): The state received for taking prev_action in prev_state\n",
    "            done (bool): Indicator that the episode is done \n",
    "        Returns: \n",
    "            action (int): the action the agent is taking given current_state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e0999",
   "metadata": {},
   "source": [
    "## Question 1 - Helper methods (3 pts)\n",
    "\n",
    "Implement the method `train_episode`, that is similar in function to the `generate_episode`, except it takes an agent as an argument instead of the policy, and simultaneously trains the agent while generating an episode. (Hint, make use of the `agent_step` method of the `Agent` class to both get an action and train the agent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c371c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_episode(agent, env):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        agent (Agent): an agent of the class Agent implemented above\n",
    "        env (DiscreteEnv): The FrozenLake environment\n",
    "    Returns:\n",
    "        states (list): the sequence of states in the generated episode\n",
    "        actions (list): the sequence of actions in the generated episode\n",
    "        rewards (list): the sequence of rewards in the generated episode \n",
    "    \"\"\"\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    done = False\n",
    "    current_state = env.reset()\n",
    "    states.append(current_state)\n",
    "    action = agent.get_current_policy()(current_state)\n",
    "    actions.append(action)\n",
    "    while not done:\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------\n",
    "        ...\n",
    "        # --------------------------\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c9e04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dea165",
   "metadata": {},
   "source": [
    "We then provide the code to train an agent using this newly written method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a831e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_control(agent_class, epsilon, step_size, run, num_episodes=100, discount=0.99):\n",
    "    agent_info = {\n",
    "        \"num_actions\": 4,\n",
    "        \"num_states\": 30,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"step_size\": step_size,\n",
    "        \"discount\": discount,\n",
    "        \"seed\": run\n",
    "        }\n",
    "    agent = agent_class()\n",
    "    agent.agent_init(agent_info)\n",
    "    \n",
    "    env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "    # Set seed\n",
    "    seed = run\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    all_returns = []\n",
    "    \n",
    "    for j in (range(num_episodes)):\n",
    "        states, actions, rewards = train_episode(agent, env)\n",
    "        all_returns.append(np.sum(rewards))\n",
    "        \n",
    "    return all_returns, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ee228",
   "metadata": {},
   "source": [
    "# Question 2 - SARSA (8 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1b754",
   "metadata": {},
   "source": [
    "## Question 2a) (5 pts)\n",
    "Implement the SARSA control algorithm. Recall the update rule given $S, A, R, S', A'$:\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S, A) + \\alpha \\left[ R+ \\gamma Q(S', A') - Q(S,A) \\right]$$\n",
    "\n",
    "And make sure to handle terminal states correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61366aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SarsaAgent(Agent):\n",
    "    \n",
    "    def agent_step(self, prev_state, prev_action, prev_reward, current_state, done):\n",
    "        \"\"\" A learning step for the agent given SARS\n",
    "        Args:\n",
    "            prev_state (int): the state observation from the enviromnents last step\n",
    "            prev_action (int): the action taken given prev_state\n",
    "            prev_reward (float): The reward received for taking prev_action in prev_state\n",
    "            current_state (int): The state received for taking prev_action in prev_state\n",
    "            done (bool): Indicator that the episode is done \n",
    "        Returns: \n",
    "            action (int): the action the agent is taking given current_state\n",
    "        \"\"\"\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------\n",
    "        ...\n",
    "        # --------------------------\n",
    "\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51f366",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 3.2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66263ca9",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2b) - Evaluating (3 pts)\n",
    "\n",
    "Let's run the SARSA algorithm on our 0 slip rate environment. We set $\\epsilon=0.5$, $\\alpha=0.1$, $\\gamma=0.99$, and run the algorithm 5 times over 2000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e51230",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running SARSA on the environment on 5 different seeds\n",
    "\n",
    "epsilon = 0.5 #@param {allow-input: true}\n",
    "step_size = 0.1 #@param {allow-input: true}\n",
    "discount = 0.99 #@param\n",
    "num_runs =  5 #@param {allow-input: true}\n",
    "num_episodes = 2000 #@param {allow-input: true}\n",
    "\n",
    "sarsa_returns = []\n",
    "sarsa_agents = []\n",
    "for i in range(num_runs):\n",
    "    returns, agent = td_control(agent_class=SarsaAgent, epsilon=epsilon, step_size=step_size, run=i, num_episodes=num_episodes, discount=discount)\n",
    "    sarsa_returns.append(returns)\n",
    "    sarsa_agents.append(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9867a",
   "metadata": {},
   "source": [
    "Now let's evaluate our agents with 0 exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d997822",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the agent with 0 exploration, i.e epsilon=0\n",
    "\n",
    "sarsa_optimal_returns = []\n",
    "for i in range(num_runs):\n",
    "    env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "    optimal_policy = make_eps_greedy_policy(sarsa_agents[i].q, epsilon=0.)\n",
    "    s, a, r = generate_episode(optimal_policy, env, render=False)\n",
    "    print('Optimal return for seed {0} is {1}'.format(i, np.sum(r)))\n",
    "    sarsa_optimal_returns.append(np.sum(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887a716",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3 - Expected Sarsa (8 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdd29e",
   "metadata": {},
   "source": [
    "### Question 3a) (5 pts)\n",
    "Implement the Expected SARSA control aglorithm. Recall the update rule:\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R + \\gamma \\mathbb{E}_{a\\sim\\pi} [Q(S', a|S')] - Q(S,A)\\right]$$\n",
    "And make sure to handle terminal states correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92d43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ExpectedSarsa\n",
    "\n",
    "class ExpectedSarsaAgent(Agent):\n",
    "    \n",
    "    def agent_step(self, prev_state, prev_action, prev_reward, current_state, done):\n",
    "        \"\"\" A learning step for the agent given SARS\n",
    "        Args:\n",
    "            prev_state (int): the state observation from the enviromnents last step\n",
    "            prev_action (int): the action taken given prev_state\n",
    "            prev_reward (float): The reward received for taking prev_action in prev_state\n",
    "            current_state (int): The state received for taking prev_action in prev_state\n",
    "            done (bool): Indicator that the episode is done \n",
    "        Returns: \n",
    "            action (int): the action the agent is taking given current_state\n",
    "        \"\"\"\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------\n",
    "        ...\n",
    "        # --------------------------\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4fa7d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 3.3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd7a7d",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3b) - Evaluating (3 pts)\n",
    "\n",
    "Let's run the expected SARSA algorithm on our 0 slip rate environment. We set $\\epsilon=0.5$, $\\alpha=0.1$, $\\gamma=0.99$, and run the algorithm 5 times over 2000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71583e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running Expected SARSA on the environment on 5 different seeds\n",
    "\n",
    "epsilon = 0.5 #@param {allow-input: true}\n",
    "step_size = 0.1 #@param {allow-input: true}\n",
    "discount = 0.99 #@param\n",
    "num_runs =  5 #@param {allow-input: true}\n",
    "num_episodes = 2000 #@param {allow-input: true}\n",
    "\n",
    "esarsa_returns = []\n",
    "esarsa_agents = []\n",
    "for i in range(num_runs):\n",
    "    returns, agent = td_control(agent_class=ExpectedSarsaAgent, epsilon=epsilon, step_size=step_size, run=i, num_episodes=num_episodes, discount=discount)\n",
    "    esarsa_returns.append(returns)\n",
    "    esarsa_agents.append(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dcd4b",
   "metadata": {},
   "source": [
    "Again, we evaluate the algorithm with 0 exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ca43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the agent with 0 exploration, i.e epsilon=0\n",
    "\n",
    "esarsa_optimal_returns = []\n",
    "for i in range(num_runs):\n",
    "    env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "    optimal_policy = make_eps_greedy_policy(esarsa_agents[i].q, epsilon=0.)\n",
    "    s, a, r = generate_episode(optimal_policy, env, render=False)\n",
    "    print('Optimal return for seed {0} is {1}'.format(i, np.sum(r)))\n",
    "    esarsa_optimal_returns.append(np.sum(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a08b3",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4 - Q-learning (8 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387266b5",
   "metadata": {},
   "source": [
    "### Question 4a) (5 pts)\n",
    "Implement the Q-learning control algorithm. Recall the update rule:\n",
    "$$Q(S,A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_a Q(S', a) - Q(S,A) \\right]$$\n",
    "\n",
    "And make sure to handle terminal states correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e996541d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Qlearning\n",
    "\n",
    "class QLearningAgent(Agent):\n",
    "    \n",
    "    def agent_step(self, prev_state, prev_action, prev_reward, current_state, done):\n",
    "        \"\"\" A learning step for the agent given SARS\n",
    "        Args:\n",
    "            prev_state (int): the state observation from the enviromnents last step\n",
    "            prev_action (int): the action taken given prev_state\n",
    "            prev_reward (float): The reward received for taking prev_action in prev_state\n",
    "            current_state (int): The state received for taking prev_action in prev_state\n",
    "            done (bool): Indicator that the episode is done \n",
    "        Returns: \n",
    "            action (int): the action the agent is taking given current_state\n",
    "        \"\"\"\n",
    "        # TO IMPLEMENT\n",
    "        # --------------------------\n",
    "        ...\n",
    "        # --------------------------\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67d019",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 3.4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926483b",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4b) -  Evaluating (3 pts)\n",
    "\n",
    "Let's run the Q-learning algorithm on our 0 slip rate environment. We set $\\epsilon=0.5$, $\\alpha=0.1$, $\\gamma=0.99$, and run the algorithm 5 times over 2000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running Q-learning on the environment on 5 different seeds\n",
    "\n",
    "epsilon = 0.5 #@param {allow-input: true}\n",
    "step_size = 0.1 #@param {allow-input: true}\n",
    "discount = 0.99 #@param\n",
    "num_runs =  5 #@param {allow-input: true}\n",
    "num_episodes = 2000 #@param {allow-input: true}\n",
    "\n",
    "q_returns = []\n",
    "q_agents = []\n",
    "for i in range(num_runs):\n",
    "    returns, agent = td_control(agent_class=QLearningAgent, epsilon=epsilon, step_size=step_size, run=i, num_episodes=num_episodes, discount=discount)\n",
    "    q_returns.append(returns)\n",
    "    q_agents.append(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36370510",
   "metadata": {},
   "source": [
    "Again, we evaluate the agent with 0 exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the agent with 0 exploration, i.e epsilon=0\n",
    "\n",
    "q_optimal_returns = []\n",
    "for i in range(num_runs):\n",
    "    env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n",
    "    optimal_policy = make_eps_greedy_policy(q_agents[i].q, epsilon=0.)\n",
    "    s, a, r = generate_episode(optimal_policy, env, render=False)\n",
    "    print('Optimal return for seed {0} is {1}'.format(i, np.sum(r)))\n",
    "    q_optimal_returns.append(np.sum(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61143b83",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 5 - Plotting everything (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afabf929",
   "metadata": {},
   "source": [
    "###### Now let us plot the learning curves of our algorithms, and their final optimal returns given a deterministic policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(stuff, window):\n",
    "    return np.convolve(stuff, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plot_many([moving_avg(r, 10) for r in sarsa_returns], label='sarsa', color='purple')\n",
    "plot_many([moving_avg(r, 10) for r in q_returns], label='q-learning', color='green')\n",
    "plot_many([moving_avg(r, 10) for r in esarsa_returns], label='expected_sarsa', color='orange')\n",
    "plt.hlines([np.mean(q_optimal_returns)], -100, 2100, color='green')\n",
    "plt.hlines([np.mean(sarsa_optimal_returns)], -100, 2100, color='purple')\n",
    "plt.hlines([np.mean(esarsa_optimal_returns)], -100, 2100, color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeea8f4",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 6 - Analysis (15 pts)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6c143",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6a) (5 pts)\n",
    "Which of the three algorithms do you expect to perform better *on-policy*, and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c25e25",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec3e20",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Quesiton 6b) (5 pts)\n",
    "Despite the worse learning curve of *Q-learning*, why does it seem to have a better optimal return during evaluation? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c90f5",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f8aa0",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Quesiton 6c) (5 pts)\n",
    "Which algorithm was quicker to run between Expected SARSA and SARSA? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d5d3",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea5502",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "otter": {
   "tests": {
    "question 0.1a": {
     "name": "question 0.1a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> for epsilon in [0, 0.01, 0.15, 0.9]:\n...     dummy_state_action_values = np.ones((16, 4))\n...     dummy_state_action_values[:, 1] = 10\n...     test_pi = make_eps_greedy_policy(dummy_state_action_values, epsilon)\n...     states = list(range(16)) * 10000\n...     all_actions = []\n...     for s in states:\n...         action = test_pi(s)\n...         all_actions.append(action)\n...     expected_rate = (1-epsilon + epsilon/4)\n...     np.testing.assert_allclose(np.sum(np.array(all_actions) == 1)/len(states), expected_rate, atol=0.015)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 0.1b": {
     "name": "question 0.1b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> true_states = np.zeros(52)\n>>> true_actions = np.ones(51) *3\n>>> true_rewards =  np.ones(51)* -1\n>>> true_rewards[-1] = -100\n>>> \n>>> env = FrozenLakeEnv(map_name=\"2x2\", slip_rate=0)\n>>> dummy_state_action_values = np.arange(16*4).reshape(16,4)\n>>> dummy_pi = make_eps_greedy_policy(dummy_state_action_values, 0.)\n>>> states, actions, rewards = generate_episode(dummy_pi, env)\n>>> \n>>> np.testing.assert_allclose(len(states), 52)\n>>> np.testing.assert_allclose(states, true_states)\n>>> np.testing.assert_allclose(actions, true_actions)\n>>> np.testing.assert_allclose(rewards, true_rewards)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1.1a": {
     "name": "question 1.1a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> dummy_states = [0, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 5, 6, 11, 11]\n>>> dummy_actions = [1, 3, 0, 0, 3, 1, 3, 1, 3, 0, 0, 1, 0, 3, 1, 0, 2, 1, 2]\n>>> dummy_rewards = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -100]\n>>> \n>>> true_answer = {(0, 1): -51.777145476657445,\n...  (5, 3): -53.44962681753415,\n...  (0, 0): -55.21013349214122,\n...  (0, 3): -59.013998329242355,\n...  (5, 0): -78.80735124999998,\n...  (5, 2): -92.2,\n...  (6, 1): -96.0,\n...  (11, 2): -100.0}\n...                                                        \n>>> visited_states_returns = fv_mc_estimation(dummy_states, dummy_actions, dummy_rewards, discount=0.95)\n>>> for sa in true_answer:\n...     assert sa in visited_states_returns\n...     np.testing.assert_allclose(true_answer[sa], visited_states_returns[sa])\n>>> assert len(true_answer) == len(visited_states_returns)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1.2a": {
     "name": "question 1.2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> for epsilon in [0, 0.01, 0.15, 0.9]:\n...     all_actions = []\n...     dummy_state_action_values = np.ones((16, 4))\n...     dummy_state_action_values[:, 1] = 10\n...     test_pi = make_eps_greedy_policy_distribution(dummy_state_action_values, epsilon)\n...     states = list(range(16))\n...     for s in states:\n...         action_dist = test_pi(s)\n...         np.testing.assert_allclose(action_dist[1], 1 - epsilon + epsilon/4)\n...         np.testing.assert_allclose(action_dist[0], epsilon/4)\n...         np.testing.assert_allclose(action_dist[2:], epsilon/4)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1.2b": {
     "name": "question 1.2b",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> dummy_states = [0, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 5]\n>>> dummy_actions = [1, 3, 0, 0, 3, 1, 3, 1, 3, 0, 0, 1, 0, 3, 1, 0]\n>>> dummy_rewards = [-1, -1, -1, -1, -1, -1, -1, -10, -1, -100, -1, -1, -1, -1, -1, -1]\n>>> \n>>> dummy_state_action_values = np.ones((12, 4))\n>>> dummy_state_action_values[:, 0] = 10\n>>> dummy_target_policy = make_eps_greedy_policy_distribution(dummy_state_action_values, epsilon=0.3)\n>>> \n>>> predicted_answer = is_mc_estimate_with_ratios(dummy_states, dummy_actions,\n...                                               dummy_rewards, dummy_target_policy, discount=0.99)\n>>> \n>>> np.testing.assert_allclose(len(predicted_answer.keys()), 2) # Only two distinct states \n...                                                             #are visited in the dummy episode\n>>> np.testing.assert_allclose(len(predicted_answer[0]), 11) # State 0 is visited 11 ttimes\n>>> \n>>> # Make sure lengths and types are correct\n>>> for k in predicted_answer:\n...     assert type(predicted_answer[k]) == list\n...     assert type(predicted_answer[k][0]) == tuple\n...     assert len(predicted_answer[k][0]) == 2\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> answer = {0: [(-2.980232238769531, 0.029802322387695312),\n...   (-2.3841857910156246, 0.02384185791015625),\n...   (-4.76837158203125, 0.0476837158203125),\n...   (-9.5367431640625, 0.095367431640625),\n...   (-19.073486328125, 0.19073486328125),\n...   (-15.2587890625, 0.152587890625),\n...   (-12.207031249999998, 0.1220703125),\n...   (-24.4140625, 0.244140625),\n...   (-48.828125, 0.48828125),\n...   (-39.0625, 0.390625),\n...   (-78.125, 0.78125)],\n...  5: [(-1.1920928955078125, 0.011920928955078125),\n...   (-7.62939453125, 0.0762939453125),\n...   (-6.103515624999999, 0.06103515625),\n...   (-19.53125, 0.1953125),\n...   (-31.25, 0.3125),\n...   (-62.49999999999999, 0.625)],\n...  6: [(-125.0, 1.25)],\n...  11: [(-50.0, 0.5)]}\n>>> \n>>> dummy_states = [0, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 5, 6, 11, 11]\n>>> dummy_actions = [1, 3, 0, 0, 3, 1, 3, 1, 3, 0, 0, 1, 0, 3, 1, 0, 2, 1, 2]\n>>> dummy_rewards = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -100]\n>>> \n>>> dummy_state_action_values = np.ones((12, 4))\n>>> dummy_state_action_values[:, 1] = 10\n>>> dummy_target_policy = make_eps_greedy_policy_distribution(dummy_state_action_values, epsilon=0.5)\n>>> \n>>> predicted_answer = is_mc_estimate_with_ratios(dummy_states, dummy_actions,\n...                                               dummy_rewards, dummy_target_policy, discount=0.99)\n>>> \n>>> for s in answer:\n...     np.testing.assert_allclose(answer[s], predicted_answer[s])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2.1a": {
     "name": "question 2.1a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # Some basic testing\n>>> dummy_states = [0, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 5]\n>>> dummy_actions = [1, 3, 0, 0, 3, 1, 3, 1, 3, 0, 0, 1, 0, 3, 1, 0]\n>>> dummy_rewards = [-1, -1, -1, -1, -1, -1, -1, -10, -1, -100, -1, -1, -1, -1, -1, -1]\n>>> \n>>> predicted_answer = ev_mc_estimate(dummy_states, dummy_actions,\n...                                               dummy_rewards, discount=0.99)\n>>> \n>>> np.testing.assert_allclose(len(predicted_answer.keys()), 2) # Only two distinct states \n...                                                             #are visited in the dummy episode\n>>> np.testing.assert_allclose(len(predicted_answer[0]), 11) # State 0 is visited 11 ttimes\n>>> \n>>> # Make sure inner type is correct\n>>> for k in predicted_answer:\n...     assert type(predicted_answer[k]) == list\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> answer = {0: [-86.11741938069248,\n...   -87.86595182194927,\n...   -88.75348668883765,\n...   -89.64998655438147,\n...   -92.57574399432471,\n...   -93.44530557527264,\n...   -92.31232075836408,\n...   -93.24476844279201,\n...   -94.18663479069899,\n...   -96.09900499,\n...   -96.059601],\n...  5: [-86.98729230372979,\n...   -93.51085251951991,\n...   -94.38919755078044,\n...   -95.13801494009999,\n...   -97.0299,\n...   -98.00999999999999],\n...  6: [-99.0],\n...  11: [-100.0]}\n>>> \n>>> dummy_states = [0, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 5, 6, 11, 11]\n>>> dummy_actions = [1, 3, 0, 0, 3, 1, 3, 1, 3, 0, 0, 1, 0, 3, 1, 0, 2, 1, 2]\n>>> dummy_rewards = [0, 0, 0, 0, 2, 0, -1, 0, -3, 0, 0, 0, 0, -1, 0, 0, 0, 0, -100]\n>>> \n>>> predicted_answer = ev_mc_estimate(dummy_states, dummy_actions, dummy_rewards, 0.99)\n>>> \n>>> np.testing.assert_allclose(len(answer), len(predicted_answer))\n>>> for k in answer:\n...     np.testing.assert_allclose(answer[k], predicted_answer[k])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2.2a": {
     "name": "question 2.2a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> answer = [ -1.49378919,  -2.96526431,  -4.41475871,  -5.84260071,\n...         -7.24911376,  -8.63461647,  -9.99942269, -11.34384158,\n...        -12.66817769, -13.97273101]\n>>> \n>>> env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n>>> \n>>> dummy_state_action_values = np.arange(30*4).reshape(30,4)\n>>> dummy_pi = make_eps_greedy_policy(dummy_state_action_values, 0.)\n>>> \n>>> dummy_state_vals = td0(dummy_pi, env, step_size=0.01, num_episodes=10)\n>>> \n>>> np.testing.assert_allclose(np.array(answer), np.array(dummy_state_vals)[:, 0])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2.3a": {
     "name": "question 2.3a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> answers = {1: [ -2.94114531,  -5.79578725,  -8.56647004, -11.25566301,\n...        -13.86576291, -16.39909598, -18.85792005, -21.24442652,\n...        -23.56074238, -25.80893201, -27.99099913, -30.10888848,\n...        -32.16448762, -34.15962861, -36.0960896 , -37.97559646,\n...        -39.7998243 , -41.57039894, -43.28889841, -44.95685431],\n...           10: [-14.13898624, -26.27886316, -36.70228456, -45.65193983,\n...        -53.33620458, -59.9339922 , -65.59891953, -70.46288356,\n...        -74.63913239, -78.22490197, -81.30368009, -83.94715019,\n...        -86.21686041, -88.16565662, -89.83891281, -91.27558753,\n...        -92.509131  , -93.56826394, -94.47764622, -95.25845106],\n...           50: [-40.10439935, -64.12517023, -78.51255523, -87.12996589,\n...        -92.29141577, -95.38289717, -97.23455853, -98.34362222,\n...        -99.00790258, -99.40577729, -99.64408674, -99.78682361,\n...        -99.87231672, -99.92352333, -99.95419384, -99.97256413,\n...        -99.98356712, -99.99015743, -99.99410473, -99.99646899]}\n>>> \n>>> \n>>> env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n>>> dummy_state_action_values = np.arange(30*4).reshape(30,4)\n>>> dummy_pi = make_eps_greedy_policy(dummy_state_action_values, 0.)\n>>> \n>>> for n in [1, 10, 50]:\n...     dummy_state_vals = tdn(dummy_pi, env, n=n, step_size=0.01, num_episodes=20)\n...     np.testing.assert_allclose(np.array(answers[n]), np.array(dummy_state_vals)[:,0], atol=1e-2)\n...     \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2.4b": {
     "name": "question 2.4b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> answers = {1: [-12.14359154, -15.00479634, -16.76559563, -18.03477267,\n...        -19.02451728, -19.83416103, -20.51819894, -21.10973223,\n...        -21.6303582 , -22.09493901, -22.51413286, -22.89583936,\n...        -23.24607351, -23.5695194 , -23.86989476, -24.15019889,\n...        -24.412886  , -24.65998929, -24.89321175, -25.11399375],\n...           10: [-50.34716411, -58.28996713, -62.60144165, -65.46553558,\n...        -67.5675138 , -69.2058335 , -70.5354492 , -71.64637001,\n...        -72.59515746, -73.41949171, -74.14563424, -74.79255955,\n...        -75.37438652, -75.90188156, -76.38342658, -76.82566406,\n...        -77.23393969, -77.61261395, -77.96528619, -78.29495916],\n...           45: [-96.29357908, -98.03394659, -98.64911857, -98.96608494,\n...        -99.16014544, -99.29150293, -99.3864821 , -99.4584462 ,\n...        -99.51491004, -99.56042854, -99.59792566, -99.62936565,\n...        -99.65611767, -99.67916568, -99.69923528, -99.71687348,\n...        -99.73250054, -99.74644477, -99.75896643, -99.77027451]}\n...            \n...                     \n>>> env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n>>> dummy_state_action_values = np.arange(30*4).reshape(30,4)\n>>> dummy_pi = make_eps_greedy_policy(dummy_state_action_values, 0.)\n>>> \n>>> \n>>> for n in [1, 10, 45]:\n...     dummy_state_vals = modified_tdn(dummy_pi, env, n=n, num_episodes=20)\n...     np.testing.assert_allclose(np.array(answers[n]), np.array(dummy_state_vals)[:,0], atol=1e-2)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2.5a": {
     "name": "question 2.5a",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> answer = [2258.25, 1808.25, 1408.25, 1058.25,  758.25,  508.25,  308.25,\n...         158.25,   58.25,    8.25,    8.25,   58.25,  158.25,  308.25,\n...         508.25,  758.25, 1058.25, 1408.25, 1808.25, 2258.25]\n>>> \n>>> test_values = np.arange(100).reshape(20, 5)\n>>> np.testing.assert_allclose(answer, value_mse(test_values))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 3.1": {
     "name": "question 3.1",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> class DummyAgent(Agent):\n...     \n...     def agent_step(self, prev_state, prev_action, prev_reward, current_state, done):\n...         \"\"\" A learning step for the agent given SARS\n...         Args:\n...             prev_state (int): the state observation from the enviromnents last step\n...             prev_action (int): the action taken given prev_state\n...             prev_reward (float): The reward received for taking prev_action in prev_state\n...             current_state (int): The state received for taking prev_action in prev_state\n...             done (bool): Indicator that the episode is done \n...         Returns: \n...             action (int): the action the agent is taking given current_state\n...         \"\"\"\n...         return 1\n>>> env = FrozenLakeEnv(map_name=\"6x5\", slip_rate=0.)\n>>> agent_info = {\n...         \"num_actions\": 4,\n...         \"num_states\": 30,\n...         \"epsilon\": 0.,\n...         \"step_size\": 0.1,\n...         \"discount\": 0.99,\n...         \"seed\": 0\n...         }\n>>> dummy_agent = DummyAgent()\n>>> dummy_agent.agent_init(agent_info)\n>>> \n>>> s_true, a_true, r_true = [0, 0, 5, 10, 15, 20, 25, 25], [0, 1, 1, 1, 1, 1, 1, 1], [-1, -1, -1, -1, -1, -1, 0]\n>>> s, a, r = train_episode(dummy_agent, env)\n>>> np.testing.assert_allclose(s, s_true)\n>>> np.testing.assert_allclose(a, a_true)\n>>> np.testing.assert_allclose(r, r_true)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 3.2a": {
     "name": "question 3.2a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> answer = [-109, -103, -142, -127, -108, -150, -105, -103, -105, -132, -150,\n...        -130, -103, -150,   -7, -112, -150, -150,  -10, -120, -130,  -35,\n...        -150,   -7, -150, -150,  -29,  -16, -150,  -11, -150, -105, -150,\n...         -21,  -43,  -13,  -41, -103, -150, -150,  -33, -150, -150,  -21,\n...        -150,  -19,  -31, -150,   -9, -138, -150,   -7, -111, -150,  -33,\n...         -43,  -23,  -13,  -45,  -13, -105,  -19,  -45,  -47, -150,  -11,\n...          -9,  -23,  -31,  -21,   -9,  -15, -132,   -5,  -48,  -11,  -25,\n...         -15,   -5,  -25,  -43,   -7,  -21,  -49,   -7,   -9,  -35,   -7,\n...         -17,   -9,  -43,  -17,   -7,   -7,  -15,  -49,  -11,  -13, -144,\n...          -5]\n>>> returns, agent = td_control(agent_class=SarsaAgent, epsilon=0, step_size=0.1,\n...            run=0, num_episodes=100, discount=0.99)\n>>> np.testing.assert_allclose(answer, returns)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 3.3a": {
     "name": "question 3.3a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> answer = [-109, -123, -105,  -10, -111, -120, -148, -136, -105, -127, -114,\n...         -10, -114, -150, -150, -107,  -36, -104, -112, -145, -111, -105,\n...        -110, -141, -150, -142, -150,  -12, -150,  -18, -122, -103,  -15,\n...        -150, -110, -123, -150,   -9, -150,  -39, -113, -127, -150,  -30,\n...         -19,  -34, -141,  -28, -106, -150, -150, -119, -109, -133, -150,\n...         -49,   -7,  -14, -150,  -34, -106, -150,  -42, -150,  -13,  -25,\n...         -47,  -26,   -7, -150,  -18,   -9,   -7, -150,  -15,   -5,  -35,\n...         -30,  -49,   -8, -150,   -9,  -13,   -7, -107, -150,   -6, -109,\n...          -5,  -17,  -31, -150, -150,   -7,  -11, -122,   -5,  -41, -129,\n...         -34]\n>>> returns, agent = td_control(agent_class=ExpectedSarsaAgent, epsilon=0.3, step_size=0.1,\n...            run=0, num_episodes=100, discount=0.99)\n>>> np.testing.assert_allclose(answer, returns)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 3.4a": {
     "name": "question 3.4a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> answer = [-109, -103, -142, -127, -108, -150, -105, -103, -105, -132, -150,\n...        -130, -103, -150,   -7, -112, -150, -150,  -10, -120, -130,  -35,\n...        -150,   -7, -150, -150,  -29,  -16, -150,  -11, -150, -105, -150,\n...         -21,  -43,  -13,  -41, -103, -150, -150,  -33, -150, -150,  -21,\n...        -150,  -19,  -31, -150,   -9, -138, -150,   -7, -111, -150,  -33,\n...         -43,  -23,  -13,  -45,  -13, -105,  -19,  -45,  -47, -150,  -11,\n...          -9,  -23,  -31,  -21,   -9,  -15, -132,   -5,  -48,  -11,  -25,\n...         -15,   -5,  -25,  -43,   -7,  -21,  -49,   -7,   -9,  -35,   -7,\n...         -17,   -9,  -43,  -17,   -7,   -7,  -15,  -49,  -11,  -13, -144,\n...          -5]\n>>> returns, agent = td_control(agent_class=QLearningAgent, epsilon=0., step_size=0.1,\n...            run=0, num_episodes=100, discount=0.99)\n>>> np.testing.assert_allclose(answer, returns)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
