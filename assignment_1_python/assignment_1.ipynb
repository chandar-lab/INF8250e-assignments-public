{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3GnyazsiOXM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Assignment 1\n",
    "\n",
    "## Instructions:\n",
    "* This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
    "* Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
    "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
    "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
    "* If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
    "\n",
    "This assignment has two questions. In question 1, you will learn:\n",
    "\n",
    "1. To understand how to formalize a marketing campaign as a multi-arm bandit problem.\n",
    "2. To implement $\\epsilon$-greedy, UCB, and Boltzmann algorithms.\n",
    "3. Understand the role of different hyper-parameters.\n",
    "\n",
    "And in question 2 you will:\n",
    "\n",
    "1. Implement and evaluate policy iteration and value iteration methods on FrozenLake environment.\n",
    "2. Compare these methods on both deterministic and stochastic versions of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q1: Marketing Campaign (75 points)\n",
    "\n",
    "Imagine you are running a marketing campaign for your website and have a set of ads to choose from. And say the objective is to show the ad with the highest click-through rate (CTR) value to drive the highest traffic possible. But you don't have any prior information about how either of these ads will perform. How do you approach this problem?\n",
    "\n",
    "Definition: CTR is the ratio of how many times an ad was clicked vs. the number of impressions. For example, if an ad has been shown 100 times and clicked 10 times, CTR = 10/100 = 0.1\n",
    "\n",
    "We will use the multi-armed bandit (MAB) method to solve this problem.\n",
    "\n",
    "In this section, you should create a class for the marketing bandit, which contains a function pull that takes the chosen ad as the input and returns if the user has clicked on the ad or not. Each ad has a true CTR (a number between zero and one), which will be used to define a Bernoulli distribution. P(click) = true_CTR, P(no click) = 1 - true_CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choice\n",
    "from scipy.stats import bernoulli\n",
    "from typing import Sequence, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(8953)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5fFSo9rgEcW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q1.a: Define your Bandit class (5 points):\n",
    "\n",
    "Most of the class has been written. Complete the pull method in such a way that: **1. Update both clicks and impressions, 2. Return if did_click is 0 or 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "192vo5l-YiWz",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bandit(object):\n",
    "\n",
    "  def __init__(self, \n",
    "               n_arm: int = 2, \n",
    "               n_pulls: int = 2000,\n",
    "               actual_ctr: list = [0.4, 0.6]\n",
    "               ):\n",
    "    self.n_arm = n_arm\n",
    "    self.n_pulls = n_pulls\n",
    "    self.init_bandit(actual_ctr=actual_ctr)\n",
    "    self.clicks = {idx: 0 for idx in range(n_arm)} # number of times an ad is clicked on\n",
    "    self.impressions = {idx: 0 for idx in range(n_arm)} # number of times an ad is chosen\n",
    "\n",
    "  def init_bandit(self,\n",
    "                  actual_ctr: list = [0.4, 0.6],\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    .inputs:\n",
    "      actual_ctr:\n",
    "          Actual click through Rate.\n",
    "    \"\"\"\n",
    "    self.actual_ctr = actual_ctr\n",
    "\n",
    "  def pull(self, a_idx: int):\n",
    "    \"\"\"Requested in part (1.)\n",
    "    .inputs:\n",
    "      a_idx: Index of action.\n",
    "    \"\"\"\n",
    "    assert a_idx < self.n_arm, \"invalid action index\"\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "    return did_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOzYYAmygGoW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Marketing campaign with three ads\n",
    "\n",
    "Let's define a marketing campaign with three ads you need to choose from with actual CTR: {0.35, 0.55, 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emYdGDYS2HvG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Problem definition\n",
    "bandit = Bandit(n_arm=3, actual_ctr=[0.35, 0.55, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDgjH3NoiOXU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q1.b1: Eps-greedy for k-armed bandit (5 points)\n",
    "\n",
    "Implement the $\\epsilon$-greedy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-t-G3rt5eCyM",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eps_greedy(\n",
    "    bandit: Bandit, \n",
    "    eps: float, \n",
    "    ctr_init_val: float = .0\n",
    "    ) -> Tuple[float, float, Sequence[float], Sequence[float]]:\n",
    "  \"\"\"\n",
    "  .inputs: \n",
    "    bandit: A bandit problem, instantiated from the above class.\n",
    "    eps: The epsilon value.\n",
    "    ctr_init_val: Here we\n",
    "      initialize all the table entries to the same value. \n",
    "  .outputs:\n",
    "    rew_record: The record of rewards at each timestep.\n",
    "    avg_ret_record: The average return up to step t, where t goes from 0 to n_pulls.\n",
    "    tot_reg_record: The  regret up to step t, where t goes from 0 to n_pulls.\n",
    "  \"\"\"\n",
    "  # init ctr (the estimates) (here we also keep a log of rewards)\n",
    "  ctr = {idx: [ctr_init_val] for idx in range(bandit.n_arm)}\n",
    "  ctr_estimated = [ctr_init_val]*bandit.n_arm\n",
    "\n",
    "\n",
    "  # the total return \n",
    "  ret = .0\n",
    "  rew_record = []\n",
    "  avg_ret_record = []\n",
    "  tot_reg_record = []\n",
    "\n",
    "  opt_ctr = np.max(bandit.actual_ctr)\n",
    "\n",
    "  for t in range(bandit.n_pulls):\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "  # calculate the regret\n",
    "  reg = bandit.n_pulls * opt_ctr - ret\n",
    "\n",
    "  return rew_record, avg_ret_record, tot_reg_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Knx7oUDmt0rf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.b2: Plotting the results (5 points)\n",
    "\n",
    "Use the driver code provided to plot:\n",
    "\n",
    "(1) the averaged reward across the $N$=100 runs as a function of the number of pulls (2000 pulls for each run) for all three $eps$ values.\n",
    "\n",
    "(2) the total regret, averaged across the $N$=100 runs as a function of the number of pulls for all three $eps$ values.\n",
    "\n",
    "for three values of $eps$=0.2, 0.05, and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "c4IeQOe9oQub",
    "outputId": "281dcec1-8402-432a-81e7-66e0b966c870",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "plt.figure(0)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "plt.figure(1)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")\n",
    "\n",
    "N = 20\n",
    "tot_reg_rec_best = 1e8\n",
    "\n",
    "for eps in [0.2, 0.05, .0]:\n",
    "  rew_rec = np.zeros(bandit.n_pulls)\n",
    "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
    "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
    "  start_time = time.time()\n",
    "  for n in range(N):\n",
    "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n = eps_greedy(bandit, eps)\n",
    "    rew_rec += np.array(rew_rec_n)\n",
    "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
    "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
    "  end_time = time.time()\n",
    "  # print(f\"time per run: {end_time - start_time}/N\")\n",
    "  # take the mean \n",
    "  rew_rec /= N\n",
    "  avg_ret_rec /= N \n",
    "  tot_reg_rec /= N\n",
    "\n",
    "  plt.figure(0)\n",
    "  plt.plot(avg_ret_rec, label=\"eps={}\".format(eps))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(1)\n",
    "  plt.plot(rew_rec[1:], label=\"eps={}\".format(eps))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(2)\n",
    "  plt.plot(tot_reg_rec, label=\"eps={}\".format(eps))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
    "        ep_greedy_dict = {\n",
    "        'reward':avg_ret_rec, \n",
    "        'regret_list':tot_reg_rec,}\n",
    "        tot_reg_rec_best = tot_reg_rec[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q1.b3: Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Explain the results from the perspective of exploration and how different $eps$ values affect the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.b4: Optimistic Initial Value (5 points)\n",
    "Run the optimistic initial value method on the same problem described above, for initial action-value estimates (1) Q1(a)=+0.6 for all a (2) Q1(a)=+100 for all a (Hint: You can use the same driver code). Compare its performance, measured by the average reward across $N$=100 runs as a function of the number of pulls, with the non-optimistic setting where Q1(a)=0 for all a. For both optimistic and non-optimistic settings, $eps$=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "\n",
    "plt.figure(4)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"reward\")\n",
    "\n",
    "plt.figure(5)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")\n",
    "\n",
    "N = 20\n",
    "for ctr_init_val in [.0, 1, 100]:\n",
    "  rew_rec = np.zeros(bandit.n_pulls)\n",
    "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
    "  for n in range(N):\n",
    "\n",
    "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n = eps_greedy(bandit, eps=0, ctr_init_val=ctr_init_val)\n",
    "\n",
    "    rew_rec += np.array(rew_rec_n)\n",
    "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
    "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
    "\n",
    "  avg_ret_rec /= N\n",
    "  rew_rec /= N\n",
    "  tot_reg_rec /= N\n",
    "  plt.figure(3)\n",
    "  plt.plot(avg_ret_rec[1:], label=\"q_init_val={}\".format(ctr_init_val))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(4)\n",
    "  plt.plot(rew_rec[1:], label=\"q_init_val={}\".format(ctr_init_val))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(5)\n",
    "  plt.plot(tot_reg_rec[1:], label=\"q_init_val={}\".format(ctr_init_val))\n",
    "  plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q1.b5: Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Explain how initial action-value estimates affect the exploration and the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJe1Q-0ueaY7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q1.c1: Upper-Confidence-Bound action selection (5 points)\n",
    "Implement the UCB algorithm on the same MAB problem as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWDDMQO4T0EO",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ucb(\n",
    "    bandit: Bandit, \n",
    "    c: float, \n",
    "    ctr_init_val: float = .0\n",
    "    ) -> Tuple[float, float, Sequence[float], Sequence[float]]:\n",
    "  \"\"\"\n",
    "  .inputs: \n",
    "    bandit: A bandit problem, instantiated from the above class.\n",
    "    c: The additional term coefficient.\n",
    "    ctr_init_val: Note the difference between the optimal ctr and ctr. Here we\n",
    "      initialize all the table entries to the same value. \n",
    "  .outputs:\n",
    "    rew_record: The record of rewards at each timestep.\n",
    "    avg_ret_record: The average return up to step t, where t goes from 0 to n_pulls.\n",
    "    tot_reg_record: The  regret up to step t, where t goes from 0 to n_pulls.\n",
    "  \"\"\"\n",
    "  # init ctr (the estimates) (here we also keep a log of rewards)\n",
    "  ctr = {idx: [ctr_init_val] for idx in range(bandit.n_arm)}\n",
    "\n",
    "  # the total return \n",
    "  ret = .0\n",
    "  rew_record = []\n",
    "  avg_ret_record = []\n",
    "  tot_reg_record = []\n",
    "\n",
    "  # the optimal action is the one with max ctr\n",
    "  opt_ctr = np.max(bandit.actual_ctr)\n",
    "\n",
    "  # for numerical stability purposes\n",
    "  eps = 1e-6\n",
    "\n",
    "  for t in range(bandit.n_pulls):\n",
    "    # Assuming to take the first arm always when there is no exploration\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "    \n",
    "  # calculate the regret\n",
    "  reg = bandit.n_pulls * opt_ctr - ret\n",
    "\n",
    "  return rew_record, avg_ret_record, tot_reg_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njJJcTBvmqq1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q1.c2: Plotting the results (5 points)\n",
    "\n",
    "Use the driver code provided to plot:\n",
    "\n",
    "(1) the averaged reward across the $N$=100 runs as a function of the number of pulls (2000 pulls for each run) for all three $eps$ values.\n",
    "\n",
    "(2) the total regret, averaged across the $N$=100 runs as a function of the number of pulls for all three $eps$ values.\n",
    "\n",
    "for three values of $c$=0.2, 1.0, and 5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "NjsHOdnfmkGP",
    "outputId": "da937883-e05f-4a9c-b407-d4ead026ab9c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(6)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "plt.figure(7)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.figure(8)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")\n",
    "\n",
    "N = 20\n",
    "tot_reg_rec_best = 1e8\n",
    "for c in [.2, 1.0, 5.0]:\n",
    "  rew_rec = np.zeros(bandit.n_pulls)\n",
    "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
    "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
    "  for n in range(N):\n",
    "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n = ucb(bandit, c)\n",
    "    rew_rec += np.array(rew_rec_n)\n",
    "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
    "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
    "\n",
    "  # take the mean \n",
    "  rew_rec /= N \n",
    "  avg_ret_rec /= N \n",
    "  tot_reg_rec /= N\n",
    "\n",
    "  plt.figure(6)\n",
    "  plt.plot(avg_ret_rec, label=\"c={}\".format(c))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(7)\n",
    "  plt.plot(rew_rec, label=\"c={}\".format(c))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(8)\n",
    "  plt.plot(tot_reg_rec, label=\"c={}\".format(c))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
    "        ucb_dict = {\n",
    "        'reward':avg_ret_rec, \n",
    "        'regret_list':tot_reg_rec,}\n",
    "        tot_reg_rec_best = tot_reg_rec[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q1.c3: Analysis (5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XamA6umJnR0d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Explain the results from the perspective of exploration and how different $c$ values affect the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1c1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wr6xzewiOXa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q1.d1: Boltzmann policy (5 points)\n",
    "\n",
    "Implement a Boltzmann policy that gets an array and temprature value ($tau$) and returns an index sampled from the Boltzmann policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLuMx_xFiOXb",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boltzmann_policy(x, tau):\n",
    "    \"\"\" Returns an index from x sampled from boltzmann distribution with temperature tau\n",
    "        Input:  x -- 1-dimensional array\n",
    "        Output: idx -- chosen index\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1d1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYLEgjSYuGoY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q1.d2: Boltzmann algorithm (5 points)\n",
    "\n",
    "Evaluate the Boltzmann algorithm on the same MAB problem as above, for three values of the parameters $tau$: $0.01$, $0.1$, and $1$. Use the driver code provided to plot their performances as measured by the average reward across $N$=100 runs as a function of the number of pulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzlVfqvGiOXb",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boltzmann(\n",
    "    bandit: Bandit,\n",
    "    tau: float = 0.1,\n",
    "    ctr_init_val: float = .0\n",
    "    ) -> Tuple[float, float, Sequence[float], Sequence[float]]:\n",
    "  \"\"\"\n",
    "  .inputs: \n",
    "    bandit: A bandit problem, instantiated from the above class.\n",
    "    c: The additional term coefficient.\n",
    "    ctr_init_val: Note the difference between the q_star and q. Here we\n",
    "      initialize all the table entries to the same value. \n",
    "  .outputs:\n",
    "    rew_record: The record of rewards at each timestep.\n",
    "    avg_ret_record: The average return up to step t, where t goes from 0 to n_pulls.\n",
    "    tot_reg_record: The  regret up to step t, where t goes from 0 to n_pulls.\n",
    "  \"\"\"\n",
    "  # init q (the estimates) (here we also keep a log of rewards)\n",
    "  ctr = {idx: [ctr_init_val] for idx in range(bandit.n_arm)}\n",
    "  ctr_estimated = [ctr_init_val]*bandit.n_arm\n",
    "\n",
    "  # the total return \n",
    "  ret = .0\n",
    "  rew_record = []\n",
    "  avg_ret_record = []\n",
    "  tot_reg_record = []\n",
    "\n",
    "  # equal q_star variance --> the optimal action is the one with max q_star_mean\n",
    "  opt_ctr = np.max(bandit.actual_ctr)\n",
    "\n",
    "  for t in range(bandit.n_pulls):\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "  # calculate the regret\n",
    "  reg = bandit.n_pulls * opt_ctr - ret\n",
    "\n",
    "  return rew_record, avg_ret_record, tot_reg_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q1.d3: Plotting the results (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "EWarVGeKiOXb",
    "outputId": "300f2cb6-e41c-489b-ab8b-a9c6cb8aa089",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(9)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "plt.figure(10)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.figure(11)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")\n",
    "\n",
    "N = 20\n",
    "tot_reg_rec_best = 1e8\n",
    "for tau in [0.01, 0.1, 1]:\n",
    "  rew_rec = np.zeros(bandit.n_pulls)\n",
    "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
    "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
    "  for n in range(N):\n",
    "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n = boltzmann(bandit, tau=tau)\n",
    "    rew_rec += np.array(rew_rec_n)\n",
    "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
    "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
    "\n",
    "  # take the mean \n",
    "  rew_rec /= N \n",
    "  avg_ret_rec /= N \n",
    "  tot_reg_rec /= N\n",
    "\n",
    "  plt.figure(9)\n",
    "  plt.plot(avg_ret_rec, label=\"tau={}\".format(tau))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(10)\n",
    "  plt.plot(rew_rec, label=\"tau={}\".format(tau))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(11)\n",
    "  plt.plot(tot_reg_rec, label=\"tau={}\".format(tau))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
    "        boltzmann_dict = {\n",
    "        'reward':avg_ret_rec, \n",
    "        'regret_list':tot_reg_rec,}\n",
    "        tot_reg_rec_best = tot_reg_rec[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q1.d4: Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Explain the role of $tau$ paramtere on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1d2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQZMb1XAiOXc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q1.f1: Final comaprison (5 points)\n",
    "Compare the performance of $\\epsilon$-greedy, UCB and Boltzmann algorithms in single plot  as measured by the average reward and total regret across $N$=100 runs as a function of the number of pulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "NQDZsBJNiOXc",
    "outputId": "7d701def-19d3-4d1f-ca98-b504b95d2eaa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(12)\n",
    "plt.plot(ep_greedy_dict[\"reward\"], label=\"e-greedy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot(ucb_dict[\"reward\"], label=\"UCB\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot(boltzmann_dict[\"reward\"], label=\"Boltzmann\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "\n",
    "plt.figure(13)\n",
    "plt.plot(ep_greedy_dict[\"regret_list\"], label=\"e-greedy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot(ucb_dict[\"regret_list\"], label=\"UCB\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot(boltzmann_dict[\"regret_list\"], label=\"Boltzmann\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q1.f2: Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Comapre all the algorithms in terms of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TrkqYbVLA_R",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q2: Dynamic Programming (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD_kdWpyiOXd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now you will implement value iteration and policy iteration for the Frozen Lake environment and evaluate them on both its deterministic and stochastic versions.\n",
    "\n",
    "Consider a variation of the FrozenLake-v0 environment from the OpenAI gym library. The agent controls the movement of a character in a grid world. Some tiles of the frozen and walkable (F), while others are holes in the lake (H) that lead to the agent falling into the water. The states are denoted as: S = {0, 1, 2, . . . , 14, 15} for a 4x4 lake. The agent starts on a starting tile $(S)$, and its goal is to find the fastest\n",
    "walkable path to a goal tile $(G)$. The agent can move in the four cardinal directions, $A$ = {left, down, right, up}, but the floor is slippery! Given a slip rate of 0 ≤ $\\epsilon$ < 1, the agent will go in a random wrong direction with probability $\\epsilon$. The reward is −1 on all transitions, except for three cases that all result in the episode terminating:\n",
    "(1) The agent falling into a hole nets the agent a reward of −100,\n",
    "(2) The agent takes over 100 steps, all the ice melts and the agent gets a reward of −200, and\n",
    "(3) The agent reaches the goal state, rewarding it a reward of 0. The discount factor for this environment\n",
    "should be set to γ = 0.99. The environment is implemented for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta61joVEMm9F",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title FrozenLake\n",
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "from gym import utils\n",
    "\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "discount=0.99\n",
    "\n",
    "MAPS = {\n",
    "    \"2x2\": [\"SF\", \"HG\"],\n",
    "    \"4x4-easy\":[\"SFFF\", \"FHFF\", \"FFFF\", \"HFFG\"],\n",
    "    \"4x4\": [\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def categorical_sample(prob_n, np_random):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "\n",
    "class DiscreteEnv(Env):\n",
    "\n",
    "    \"\"\"\n",
    "    Has the following members\n",
    "    - nS: number of states\n",
    "    - nA: number of actions\n",
    "    - P: transitions (*)\n",
    "    - isd: initial state distribution (**)\n",
    "    (*) dictionary of lists, where\n",
    "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "    (**) list or array of length nS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nS, nA, P, isd, max_length=100):\n",
    "        self.P = P\n",
    "        self.isd = isd\n",
    "        self.lastaction = None  # for rendering\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "\n",
    "        self.seed()\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        self.t = 0\n",
    "        return int(self.s)\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, d = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "        if self.t >= self.max_length:\n",
    "            d = True\n",
    "            r = -200\n",
    "        self.t += 1\n",
    "        return (int(s), r, d, {\"prob\": p})\n",
    "\n",
    "def generate_random_map(size=8, p=0.8):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0, 0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == \"G\":\n",
    "                        return True\n",
    "                    if res[r_new][c_new] != \"H\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        res[0][0] = \"S\"\n",
    "        res[-1][-1] = \"G\"\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "\n",
    "class FrozenLakeEnv(DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"ansi\"]}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\", slip_rate=0.5):\n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc, dtype=\"c\")\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "        self.slip_rate=slip_rate\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b\"S\").astype(\"float64\").ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row * ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            newrow, newcol = inc(row, col, action)\n",
    "            newstate = to_s(newrow, newcol)\n",
    "            newletter = desc[newrow, newcol]\n",
    "            done = bytes(newletter) in b\"GH\"\n",
    "            # reward = float(newletter == b\"G\")\n",
    "            reward = -1\n",
    "            # if newletter == b\"H\":\n",
    "            #     reward = -100\n",
    "            done = False\n",
    "            return newstate, reward, done\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter == b\"G\":\n",
    "                        li.append((1, s, 0, True))\n",
    "                    elif letter == b'H':\n",
    "                        li.append((1, s, -100, True))\n",
    "                    else:\n",
    "                        if slip_rate > 0:\n",
    "                            li.append((slip_rate/2.0, *update_probability_matrix(row, col, (a - 1) % 4)))\n",
    "                            li.append((1 - slip_rate, *update_probability_matrix(row, col, a)))\n",
    "                            li.append((slip_rate/2.0, *update_probability_matrix(row, col, (a + 1) % 4)))\n",
    "                        else:\n",
    "                            li.append((1, *update_probability_matrix(row, col, a)))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        outfile = StringIO() if mode == \"ansi\" else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\n",
    "                \"  ({})\\n\".format([\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction])\n",
    "            )\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(\"\".join(line) for line in desc) + \"\\n\")\n",
    "\n",
    "        if mode != \"human\":\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2.a Generate an episode (5 points):\n",
    "Complete the code below to generate an episode and return total rewards and number of steps during the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_episode(policy, env, render=False):\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    current_state = env.reset()\n",
    "    steps = 0\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "    return episode_return, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuH3jpUZLpNN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2.b Tabular Policy iteration Implementation (15 points):\n",
    "Implement policy_evaluation, policy_improvement and policy_iteration. policy_iteration runs the policy for 300 iterations while running  policy_evaluation and policy_improvement evey 5 episodes. Return the optimal value function and the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2Yq66n6GpDA",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#title\n",
    "###########################################\n",
    "##### ALL FUCTIONS FOR POLICY ITERATION ###\n",
    "###########################################\n",
    "\n",
    "def greedify_policy(Q_vals, tolerance=1e-9):\n",
    "    '''\n",
    "    chooses the optimal action at each step such that the agent moves to the state with the maximum value\n",
    "    arguments:\n",
    "        Q_vals - vector that stores the values of next states for each action\n",
    "    return:\n",
    "        greedy_policy - the greedified policy, a matrix containing prob of actions\n",
    "    '''\n",
    "    max_Q = np.max(Q_vals)\n",
    "    greedy_policy = np.zeros(len(Q_vals))\n",
    "    for action in range(len(Q_vals)):\n",
    "        if np.abs(Q_vals[action]-max_Q)<=tolerance:\n",
    "            greedy_policy[action] = 1.\n",
    "    greedy_policy /= np.sum(greedy_policy)\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "def policy_evaluation(environment, policy, theta=1e-8):\n",
    "    '''\n",
    "    Computes the value function for a given policy\n",
    "    arguments:\n",
    "        environment - class instance of an MDP environment\n",
    "        policy - the policy for which value function is to be evaluated\n",
    "        theta - tolerance level to decide convergence\n",
    "    return:\n",
    "        Value_fn - the estimated value of each state, evaluated for given policy\n",
    "    '''\n",
    "    Value_fn = np.zeros(environment.observation_space.n)\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "    return Value_fn\n",
    "\n",
    "def policy_improvement(environment, Value_fn, curr_policy):\n",
    "    '''\n",
    "    Decides a new policy based on given value function\n",
    "    arguments:\n",
    "        environment - class instance of an MDP environment\n",
    "        Value_fn - estimated value of each state\n",
    "        curr_policy - present policy\n",
    "    return:\n",
    "        curr_policy - updated policy\n",
    "        policy_stable - True/False flag indicating if policy was changed\n",
    "    '''\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "    return curr_policy, policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration_with_performance(environment, theta=1e-8, iterations=90, train_steps=1, test_steps=5):\n",
    "    '''\n",
    "    Runs the tabular policy iteration algorithm for given iterations and returns cumulative reward and total steps at each iteration\n",
    "    arguments:\n",
    "        environment - class instance of an MDP environment\n",
    "        theta - tolerance level to decide convergence\n",
    "        iterations - number of iterations to run the algorithm\n",
    "        train_steps - number of consecutive iterations to update the value function and policy\n",
    "        test_steps - number of consecutive iterations to evaluate the current policy in the environment\n",
    "    return:\n",
    "        cumulative_reward_array - array of cumulative reward at each step (both train and test included)\n",
    "        total_steps_array - array of total number of steps at each step (both train and test included)\n",
    "    '''\n",
    "    n_steps = train_steps+test_steps\n",
    "    V = np.zeros(environment.nS)\n",
    "    pi = np.random.rand(environment.nS,environment.nA)\n",
    "    pi = pi/pi.sum(axis=1,keepdims=True)\n",
    "    cumulative_reward_array = np.zeros(iterations)\n",
    "    total_steps_array = np.zeros(iterations)\n",
    "    for iter in range(iterations):\n",
    "        cumulative_reward_array[iter], total_steps_array[iter] = generate_episode(pi, environment)\n",
    "        if (iter%n_steps)>=test_steps:\n",
    "            # ----------------------------------------------\n",
    "            ...\n",
    "            # ----------------------------------------------\n",
    "\n",
    "    return cumulative_reward_array, total_steps_array, V, pi\n",
    "\n",
    "def plot_performance_policy_iteration(env, iterations=90, train_steps=1, test_steps=5, num_seeds=5):\n",
    "    '''\n",
    "    Plots the performance metrics of tabular policy iteration algorithm\n",
    "    arguments:\n",
    "        env - MDP environment\n",
    "        iterations - number of iterations to run the algorithm\n",
    "        train_steps - number of consecutive iterations to update the value function and policy\n",
    "        test_steps - number of consecutive iterations to evaluate the current policy in the environment\n",
    "    return:\n",
    "        \n",
    "    '''\n",
    "    cumulative_reward_array_train_plot = []\n",
    "    cumulative_reward_array_test_plot = []\n",
    "    total_steps_array_train_plot = []\n",
    "    total_steps_array_test_plot = []\n",
    "    for seed in range(num_seeds):\n",
    "        np.random.seed(seed)\n",
    "        env.seed(seed)\n",
    "        cumulative_reward_array, total_steps_array, V, pi = policy_iteration_with_performance(env,theta=1e-3,iterations=iterations,train_steps=train_steps,test_steps=test_steps)\n",
    "        all_idx = np.array(list(range(iterations)))\n",
    "        train_idx = np.array([i for i in range(iterations) if i%(train_steps+test_steps)>=test_steps])\n",
    "        test_idx = np.setdiff1d(all_idx,train_idx)\n",
    "        cumulative_reward_array_train = cumulative_reward_array[train_idx]\n",
    "        cumulative_reward_array_test = cumulative_reward_array[test_idx].reshape(-1,test_steps).mean(axis=1)\n",
    "        total_steps_array_train = total_steps_array[train_idx]\n",
    "        total_steps_array_test = total_steps_array[test_idx].reshape(-1,test_steps).mean(axis=1)\n",
    "        cumulative_reward_array_train_plot.append(cumulative_reward_array_train)\n",
    "        cumulative_reward_array_test_plot.append(cumulative_reward_array_test)\n",
    "        total_steps_array_train_plot.append(total_steps_array_train)\n",
    "        total_steps_array_test_plot.append(total_steps_array_test)\n",
    "        \n",
    "    cumulative_reward_array_train_plot = np.array(cumulative_reward_array_train_plot)\n",
    "    cumulative_reward_array_test_plot = np.array(cumulative_reward_array_test_plot)\n",
    "    cumulative_reward_array_train_mean = np.mean(cumulative_reward_array_train_plot,axis=0)\n",
    "    cumulative_reward_array_train_std = np.std(cumulative_reward_array_train_plot,axis=0)\n",
    "    cumulative_reward_array_test_mean = np.mean(cumulative_reward_array_test_plot,axis=0)\n",
    "    cumulative_reward_array_test_std = np.std(cumulative_reward_array_test_plot,axis=0)\n",
    "    total_steps_array_train_plot = np.array(total_steps_array_train_plot)\n",
    "    total_steps_array_test_plot = np.array(total_steps_array_test_plot)\n",
    "    total_steps_array_train_mean = np.mean(total_steps_array_train_plot,axis=0)\n",
    "    total_steps_array_train_std = np.std(total_steps_array_train_plot,axis=0)\n",
    "    total_steps_array_test_mean = np.mean(total_steps_array_test_plot,axis=0)\n",
    "    total_steps_array_test_std = np.std(total_steps_array_test_plot,axis=0)\n",
    "    \n",
    "    plt.figure(14);\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.plot(np.linspace(1,len(cumulative_reward_array_test_mean),len(cumulative_reward_array_test_mean)),cumulative_reward_array_test_mean,label='Mean cumulative reward of an episode')\n",
    "    plt.fill_between(np.linspace(1,len(cumulative_reward_array_test_mean),len(cumulative_reward_array_test_mean)),cumulative_reward_array_test_mean-cumulative_reward_array_test_std,cumulative_reward_array_test_mean+cumulative_reward_array_test_std,alpha=0.4)\n",
    "    optimal_reward_test = np.max(cumulative_reward_array_test_plot,axis=1)\n",
    "    plt.axhline(y=np.mean(optimal_reward_test),color='k',linestyle='--',label='Optimal performance')\n",
    "    plt.axhspan(np.mean(optimal_reward_test)-np.std(optimal_reward_test),np.mean(optimal_reward_test)+np.std(optimal_reward_test),alpha=0.2,color='k')\n",
    "    plt.title('Cumulative Reward during testing vs episodes with Policy iteration',size=12)\n",
    "    plt.ylabel('Cumulative reward',size=12)\n",
    "    plt.xlabel('Episodes',size=12)\n",
    "#     plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(15);\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.plot(np.linspace(1,len(total_steps_array_test_mean),len(total_steps_array_test_mean)),total_steps_array_test_mean,label='Mean steps of an episode')\n",
    "    plt.fill_between(np.linspace(1,len(total_steps_array_test_mean),len(total_steps_array_test_mean)),total_steps_array_test_mean-total_steps_array_test_std,total_steps_array_test_mean+total_steps_array_test_std,alpha=0.4)\n",
    "    optimal_steps_test = np.min(total_steps_array_test_plot,axis=1)\n",
    "    plt.axhline(y=np.mean(optimal_steps_test),color='k',linestyle='--',label='Optimal performance')\n",
    "    plt.axhspan(np.mean(optimal_steps_test)-np.std(optimal_steps_test),np.mean(optimal_steps_test)+np.std(optimal_steps_test),alpha=0.2,color='k')\n",
    "    plt.title('Total steps per episode during testing vs episodes with Policy iteration',size=12)\n",
    "    plt.ylabel('Total steps/episode',size=12)\n",
    "    plt.xlabel('Episodes',size=12)\n",
    "#     plt.xscale('log')\n",
    "    plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7yWZjv4iOXf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2.c: Tabular Value Iteration Implementation (10 points)\n",
    "Implement value_iteration algorithm and evaluate it for 300 iterations while performing value iteration updates every 5 iteration. Return the optimal value function and the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSB9ysPbiOXf",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "###########################################\n",
    "##### ALL FUCTIONS FOR VALUE ITERATION ###\n",
    "###########################################\n",
    "\n",
    "def value_iteration_with_performance(environment, theta=1e-8, iterations=90, train_steps=1, test_steps=5):\n",
    "    '''\n",
    "    Runs the tabular value iteration algorithm for given iterations and returns cumulative reward and total steps at each iteration\n",
    "    arguments:\n",
    "        environment - class instance of an MDP environment\n",
    "        theta - tolerance level to decide convergence\n",
    "        iterations - number of iterations to run the algorithm\n",
    "        train_steps - number of consecutive iterations to update the value function and policy\n",
    "        test_steps - number of consecutive iterations to evaluate the current policy in the environment\n",
    "    return:\n",
    "        cumulative_reward_array - array of cumulative reward at each step (both train and test included)\n",
    "        total_steps_array - array of total number of steps at each step (both train and test included)\n",
    "    '''\n",
    "    n_steps = train_steps+test_steps\n",
    "    V = np.zeros(environment.nS)\n",
    "    pi = np.random.rand(environment.nS,environment.nA)\n",
    "    pi = pi/pi.sum(axis=1,keepdims=True)\n",
    "    cumulative_reward_array = np.zeros(iterations)\n",
    "    total_steps_array = np.zeros(iterations)\n",
    "    for iter in range(iterations):\n",
    "        cumulative_reward_array[iter], total_steps_array[iter] = generate_episode(pi, environment)\n",
    "        if (iter%n_steps)>=test_steps:\n",
    "            # ----------------------------------------------\n",
    "            ...\n",
    "            # ----------------------------------------------\n",
    "\n",
    "    return cumulative_reward_array, total_steps_array, V, pi\n",
    "\n",
    "def plot_performance_value_iteration(env, iterations=90, train_steps=1, test_steps=5, num_seeds=5):\n",
    "    '''\n",
    "    Plots the performance metrics of tabular value iteration algorithm\n",
    "    arguments:\n",
    "        env_class - MDP environment class\n",
    "        iterations - number of iterations to run the algorithm\n",
    "        train_steps - number of consecutive iterations to update the value function and policy\n",
    "        test_steps - number of consecutive iterations to evaluate the current policy in the environment\n",
    "    return:\n",
    "        \n",
    "    '''\n",
    "    cumulative_reward_array_train_plot = []\n",
    "    cumulative_reward_array_test_plot = []\n",
    "    total_steps_array_train_plot = []\n",
    "    total_steps_array_test_plot = []\n",
    "    for seed in range(num_seeds):\n",
    "        np.random.seed(seed)\n",
    "        env.seed(seed)\n",
    "        cumulative_reward_array, total_steps_array, V, pi = value_iteration_with_performance(env,theta=1e-3, iterations=iterations)\n",
    "        all_idx = np.array(list(range(iterations)))\n",
    "        train_idx = np.array([i for i in range(iterations) if i%(train_steps+test_steps)>=test_steps])\n",
    "        test_idx = np.setdiff1d(all_idx,train_idx)\n",
    "        cumulative_reward_array_train = cumulative_reward_array[train_idx]\n",
    "        cumulative_reward_array_test = cumulative_reward_array[test_idx].reshape(-1,test_steps).mean(axis=1)\n",
    "        total_steps_array_train = total_steps_array[train_idx]\n",
    "        total_steps_array_test = total_steps_array[test_idx].reshape(-1,test_steps).mean(axis=1)\n",
    "        cumulative_reward_array_train_plot.append(cumulative_reward_array_train)\n",
    "        cumulative_reward_array_test_plot.append(cumulative_reward_array_test)\n",
    "        total_steps_array_train_plot.append(total_steps_array_train)\n",
    "        total_steps_array_test_plot.append(total_steps_array_test)\n",
    "    cumulative_reward_array_train_plot = np.array(cumulative_reward_array_train_plot)\n",
    "    cumulative_reward_array_test_plot = np.array(cumulative_reward_array_test_plot)\n",
    "    cumulative_reward_array_train_mean = np.mean(cumulative_reward_array_train_plot,axis=0)\n",
    "    cumulative_reward_array_train_std = np.std(cumulative_reward_array_train_plot,axis=0)\n",
    "    cumulative_reward_array_test_mean = np.mean(cumulative_reward_array_test_plot,axis=0)\n",
    "    cumulative_reward_array_test_std = np.std(cumulative_reward_array_test_plot,axis=0)\n",
    "    total_steps_array_train_plot = np.array(total_steps_array_train_plot)\n",
    "    total_steps_array_test_plot = np.array(total_steps_array_test_plot)\n",
    "    total_steps_array_train_mean = np.mean(total_steps_array_train_plot,axis=0)\n",
    "    total_steps_array_train_std = np.std(total_steps_array_train_plot,axis=0)\n",
    "    total_steps_array_test_mean = np.mean(total_steps_array_test_plot,axis=0)\n",
    "    total_steps_array_test_std = np.std(total_steps_array_test_plot,axis=0)\n",
    "    plt.figure(16);\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.plot(np.linspace(1,len(cumulative_reward_array_test_mean),len(cumulative_reward_array_test_mean)),cumulative_reward_array_test_mean,label='Mean cumulative reward of an episode')\n",
    "    plt.fill_between(np.linspace(1,len(cumulative_reward_array_test_mean),len(cumulative_reward_array_test_mean)),cumulative_reward_array_test_mean-cumulative_reward_array_test_std,cumulative_reward_array_test_mean+cumulative_reward_array_test_std,alpha=0.4)\n",
    "    optimal_reward_test = np.max(cumulative_reward_array_test_plot,axis=1)\n",
    "    plt.axhline(y=np.mean(optimal_reward_test),color='k',linestyle='--',label='Optimal performance')\n",
    "    plt.axhspan(np.mean(optimal_reward_test)-np.std(optimal_reward_test),np.mean(optimal_reward_test)+np.std(optimal_reward_test),alpha=0.2,color='k')\n",
    "    plt.title('Cumulative Reward during testing vs episodes with Value iteration',size=12)\n",
    "    plt.ylabel('Cumulative reward',size=12)\n",
    "    plt.xlabel('Episodes',size=12)\n",
    "#     plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.figure(17);\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.plot(np.linspace(1,len(total_steps_array_test_mean),len(total_steps_array_test_mean)),total_steps_array_test_mean,label='Mean steps of an episode')\n",
    "    plt.fill_between(np.linspace(1,len(total_steps_array_test_mean),len(total_steps_array_test_mean)),total_steps_array_test_mean-total_steps_array_test_std,total_steps_array_test_mean+total_steps_array_test_std,alpha=0.4)\n",
    "    optimal_steps_test = np.min(total_steps_array_test_plot,axis=1)\n",
    "    plt.axhline(y=np.mean(optimal_steps_test),color='k',linestyle='--',label='Optimal performance')\n",
    "    plt.axhspan(np.mean(optimal_steps_test)-np.std(optimal_steps_test),np.mean(optimal_steps_test)+np.std(optimal_steps_test),alpha=0.2,color='k')\n",
    "    plt.title('Total steps per episode during testing vs episodes with Value iteration',size=12)\n",
    "    plt.ylabel('Total steps/episode',size=12)\n",
    "    plt.xlabel('Episodes',size=12)\n",
    "#     plt.xscale('log')\n",
    "    plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMVBdTs-L_e-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q2.d1: PI and VI experiments (10 points)\n",
    "Run both methods on the 4x4 FrozenLake environment with $\\textit{slip_rate=0}$ for 5 independent seeds and plot the cumulative reward and total steps per episode for each method using *plot_performance_policy_iteration* and *plot_performance_value_iteration* functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RiBDGj5V1pXt",
    "outputId": "8f043a6f-7786-4ef9-fd5a-fcf35582f34b",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env=FrozenLakeEnv(map_name=\"4x4\", slip_rate=0)\n",
    "# PI\n",
    "# ----------------------------------------------\n",
    "...\n",
    "# ----------------------------------------------\n",
    "\n",
    "# VI\n",
    "# ----------------------------------------------\n",
    "...\n",
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPlbEUbgqSyr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2.d2: PI and VI visualizations (5 points)\n",
    "Visualize the value function for both methods using policy_iteration_with_performance and value_iteration_with_performance functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3uD7Aku94Br",
    "outputId": "55b7150d-aa51-4c40-a90a-bbb4a0010d86",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "9AVFqfBfiOXg",
    "outputId": "11070460-cf8c-4cf4-a1f7-21785aa7f64e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(V.reshape(int(np.sqrt(env.nS)),int(np.sqrt(env.nS))), vmin=-10); plt.colorbar(); # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O51OugIFqUZa",
    "outputId": "5818dd5f-5ca4-401f-d8ee-52b17bb3515a",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Ui4QviTyKXTR",
    "outputId": "4e750be0-f491-4f34-93dc-aef566fa507c",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(V.reshape(int(np.sqrt(env.nS)),int(np.sqrt(env.nS))), vmin=-10); plt.colorbar(); # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2.d3: Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What do you observe in these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB5hIYZHiOXh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q2.e1: Stochastic FrozenLake (10 points)\n",
    "Rerun the previous part using 4x4 FrozenLake environment but this time with $\\textit{slip_rate=0.1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XwWbMNvniOXh",
    "outputId": "4d8eac46-718c-4007-d931-10f3762b4e25",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the env=...\n",
    "# ----------------------------------------------\n",
    "...\n",
    "# ----------------------------------------------\n",
    "\n",
    "# PI\n",
    "# ----------------------------------------------\n",
    "...\n",
    "# ----------------------------------------------\n",
    "\n",
    "# VI\n",
    "# ----------------------------------------------\n",
    "...\n",
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q2.e2: Analysis (5 points)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Compare the curves with the case when environment was deterministic.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A1_RL_no_solution-ToTest.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "otter": {
   "tests": {
    "question 1a": {
     "name": "question 1a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def temp_call(dummy_bandit, arm):\n...     dummy_bandit.pull(arm)\n>>> \n>>> dummy_bandit = Bandit(n_arm=3, actual_ctr=[0.0, 1, 0.0])\n>>> test_arms = [0, 2, 1, 0, 0, 0, 2]\n>>> for arm in test_arms:\n...     temp_call(dummy_bandit, arm)\n>>> np.testing.assert_allclose(dummy_bandit.impressions[0],4)\n>>> np.testing.assert_allclose(dummy_bandit.clicks[1],1)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1b1": {
     "name": "question 1b1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> N=20\n>>> dummy_bandit = Bandit(n_arm=3, n_pulls=1000, actual_ctr=[1, 0.0, 0.0])\n>>> for n in range(N):\n...     rew_rec_n, avg_ret_rec_n, tot_reg_rec_n = eps_greedy(dummy_bandit, eps=1)\n...     np.testing.assert_allclose(tot_reg_rec_n[-1],660,atol=60)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1c1": {
     "name": "question 1c1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> N=10\n>>> for n in range(N):\n...     dummy_bandit = Bandit(n_arm=3, n_pulls=1000, actual_ctr=[0, 1.0, 0])\n...     rew_rec_n, avg_ret_rec_n, tot_reg_rec_n = ucb(dummy_bandit, c=1)\n...     np.testing.assert_allclose(tot_reg_rec_n[-1],10, atol=2)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1d1": {
     "name": "question 1d1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> N=1000\n>>> dummy_x = [20, 100, 60]\n>>> tmp_sum = []\n>>> for n in range(N):\n...     dummy_idx = boltzmann_policy(dummy_x, tau=1e10) # uniform sampling\n...     tmp_sum.append(dummy_x[dummy_idx])\n>>> np.testing.assert_allclose(np.mean(tmp_sum), 60, atol=5)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1d2": {
     "name": "question 1d2",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> dummy_bandit = Bandit(n_arm=3, n_pulls=1000, actual_ctr=[0, 1.0, 0])\n>>> N=10\n>>> for n in range(N):\n...     rew_rec_n, avg_ret_rec_n, tot_reg_rec_n = boltzmann(dummy_bandit, tau=1e8) # uniform sampling\n...     np.testing.assert_allclose(tot_reg_rec_n[-1],666.0, atol=50)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2a": {
     "name": "question 2a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> dummy_env = FrozenLakeEnv(map_name=\"2x2\", slip_rate=0)\n>>> dummy_pi = np.array([[0, 0, 1, 0],[0,1,0,0],[0,0,1,0],[0,1,0,0]])\n>>> ret, steps = generate_episode(dummy_pi, dummy_env)\n>>> np.testing.assert_allclose(ret,-2)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2b": {
     "name": "question 2b",
     "points": [
      5,
      10
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> dummy_env=FrozenLakeEnv(map_name=\"2x2\", slip_rate=0)\n>>> dummy_pi = np.array([[0, 0, 1, 0],[0,1,0,0],[0,0,1,0],[0,1,0,0]])\n>>> dummy_v = policy_evaluation(dummy_env, dummy_pi)\n>>> np.testing.assert_allclose(dummy_v,[-1.99, -1, -100, 0])\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> \n>>> dummy_env=FrozenLakeEnv(map_name=\"2x2\", slip_rate=0)\n>>> dummy_v = np.zeros(dummy_env.nS)\n>>> dummy_pi = np.random.rand(dummy_env.nS,dummy_env.nA)\n>>> \n>>> dummy_pi = dummy_pi/dummy_pi.sum(axis=1,keepdims=True)\n>>> \n>>> for iter in range(100):\n...   dummy_v = policy_evaluation(dummy_env,dummy_pi,1e-8)\n...   dummy_pi, dummy_policy_stable = policy_improvement(dummy_env,dummy_v,dummy_pi)\n>>> \n>>> np.testing.assert_allclose(dummy_pi[:2, :],np.array([[0, 0, 1, 0], [0, 1, 0, 0]]))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2c": {
     "name": "question 2c",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> dummy_env=FrozenLakeEnv(map_name=\"2x2\", slip_rate=0)\n>>> \n>>> dummy_v = np.zeros(dummy_env.nS)\n>>> _, _, dummy_v, dummy_pi = value_iteration_with_performance(dummy_env)\n>>> np.testing.assert_allclose(dummy_v,[-1.99, -1, -100, 0])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}