{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 4 -- Learning to use RL Hive library (140 Points Total)**"
      ],
      "metadata": {
        "id": "Nqs0Bc1Gzt08"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNdO45rrlObT"
      },
      "source": [
        "# Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om5VhzMDqsp8"
      },
      "source": [
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
        "* Part of this assignment will be autograded by gradescope. You can use it as \n",
        "immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
        "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
        "* If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
        "* Please use the following branch of RLHive for your experiments: <https://github.com/chandar-lab/RLHive/tree/rl_class>\n",
        "\n",
        "\n",
        "\n",
        "This assignment has 4 parts. The goals of these parts are:\n",
        "\n",
        "* Question 1: Understand the usage of RL Hive library and run DQN on minigrid environment across multiple seeds, report observations/intuitions while varying replay buffer sizes in DQN.\n",
        "* Question 2: Run DQN on minigrid environment across multiple seeds, report observations/intuitions while varying target network update frequencies.\n",
        "* Question 3: Run DQN agent on minigrid environment across multiple seeds for varying values of update horizons, report observations/intuitions while observing performance changes.\n",
        "* Question 4: Run ablation studies on Rainbow algorithm to observe the contribution of each baseline component used in the algorithm. Report observations and intuitions regarding this observation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-WlgAw0FDXF"
      },
      "source": [
        "#  Example -  Training DQN agent on Minigrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amrNQCUtqPRw"
      },
      "source": [
        "In RLHive, there are config samples for DQN agents, for different environment including, Atari, Gym, Hanabi, Marlgrid, Minatar, and Minigrid. \n",
        "\n",
        "In this tutorial, we are using a simple environment from Minigrid called `MiniGrid-Empty-5x5-v0` which is an empty room and the objective of the agent is to reach the green square. The reward provided to the agent on reaching the green square is penalized depending on how many steps it took to reach there.\n",
        "\n",
        "<!-- About the environment - We are using `MiniGrid-Empty-5x5-v0` environment which is an empty room and the objective of the agent is to reach the green sqare, the reward provided to the agent on reaching the green square is penalized and dependent on how many steps it took to reach there. -->\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"data:image/png;base64,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\" width=\"400\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4tPX0_QpbOB"
      },
      "outputs": [],
      "source": [
        "## used for updating config.yaml files \n",
        "!pip install ruamel.yaml &> /dev/null\n",
        "!pip install opencv-python\n",
        "!pip install git+https://github.com/chandar-lab/RLHive.git@rl_class &> /dev/null\n",
        "\n",
        "## change the gym version, because of breaking changes in gym 0.25\n",
        "!pip install gym==0.21.0 &> /dev/null\n",
        "!pip install pygame &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mO8egKztQkq"
      },
      "outputs": [],
      "source": [
        "!pip install gym_minigrid==1.0.2 &> /dev/null\n",
        "!wget https://www.dropbox.com/s/nyqx3v5atit9nex/configs.zip\n",
        "!unzip \"/content/configs.zip\" -d \"/content/\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR3h0n3NxhD-"
      },
      "source": [
        "Now that we have installed RLHive library and the dependencies, we will now see how to run experiments using this library. There are several ways to run an experiment with RLHive. But, an important component that is requirerd to run an experiment on RLHive is the Config file. In the following exercises we provide the config files with certain configurations. These config files need to be used while running your experiments for this assignment. Please also refer to the Hive [documentation](https://rlhive.readthedocs.io/en/stable/index.html#) if you would like to learn more about the library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao8lbOdC0RhW"
      },
      "source": [
        "A typical config file in RLHive looks is an YAML file and has the following structure:\n",
        "\n",
        "```\n",
        "agent:\n",
        "  name: DQNAgent\n",
        "  kwargs:\n",
        "    representation_net:\n",
        "      name: MLPNetwork\n",
        "      kwargs:\n",
        "        hidden_units: [256, 256]\n",
        "    discount_rate: .9\n",
        "    replay_buffer:\n",
        "      name: CircularReplayBuffer\n",
        "    reward_clip: 1.0\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgvqV5iF5gtc"
      },
      "source": [
        "<!-- Before we train the agent, first step is to install the environment dependencies.  -->\n",
        "<!-- ## Run RLHive\n",
        "The simplest way to run RLhive (for single agent) is enough to run:\n",
        "\n",
        " `hive_single_agent_loop -p PATH/TO/CONFIG_FILE`.  **ðŸ˜ƒ**\n",
        " \n",
        " In this case, we can just simply use the ready config files for DQN agent and gyn environemnts:\n",
        "\n",
        " ` hive_single_agent_loop -p gym/dqn.yml`.\n",
        "\n",
        "By running this line, the learning curves of the agent have been shown in wandb in addition to the log files.   -->\n",
        "\n",
        "For our following experiments the files needed to run each experiment would be located in the following folder structure. Also the results from your experiments would be saved in the respective folder locations as well. Below, we have also shown an example of how to train a DQN agent using the config files along with certain modifyable parameters.\n",
        "```\n",
        "experiments \n",
        "|-- Q1 (Buffer Size)\n",
        "    |-- buffer_capacity_100\n",
        "    |-- buffer_capacity_10000\n",
        "    |-- buffer_capacity_100000 \n",
        "|-- Q2 (Target update Frequency)\n",
        "    |-- update_freq_1\n",
        "    |-- update_freq_100\n",
        "    |-- update_freq_1000\n",
        "    |-- update_freq_10000\n",
        "|-- Q3 (TD Update Horizon)\n",
        "    |-- n-step updates DQN\n",
        "        |-- n_step_1\n",
        "        |-- n_step_3\n",
        "        |-- n_step_10\n",
        "|-- Q4 (Ablation Studies)\n",
        "    |-- Rainbow\n",
        "        |-- Understand contribution of each baseline in Rainbow\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Eo-dHxFNfc"
      },
      "outputs": [],
      "source": [
        "# Training the DQN Agent\n",
        "# num_seeds=2\n",
        "# for idx in range(num_seeds):\n",
        "#   !hive_single_agent_loop --config  config.yml --run_name 'MiniGrid-FourRooms-v0-image-dqn-{idx}' --train_steps 50000  --agent.hidden_units [128, 128] --agent.epsilon_schedule.end_value 0.5 --save_dir 'experiment/minigrid_four_rooms_test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk1xStYJhIJG"
      },
      "outputs": [],
      "source": [
        "# num_seeds=2\n",
        "# for idx in range(num_seeds):\n",
        "#   !hive_single_agent_loop --config  config.yml --run_name 'minigrid-empty-16x16-image-dqn-{idx}' --train_steps 50000  --agent.hidden_units [128, 128] --agent.epsilon_schedule.end_value 0.5 --save_dir 'experiment/minigrid16*16_test'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi6WMj1ea6EG"
      },
      "source": [
        "Now, as you probably realised already, to use a DQN agent with different parameters or hyper-parameters, e.g. different layers with different sizes, adding or removing target network, different schedules for updating the target network, etc. all we need to do is to create the desired config file and change its parameters accordingly and then train the agent using this config.\n",
        "\n",
        "<!-- In the next step we can train this agent using the training loop of the class SingleAgentRunner.\n",
        "\n",
        "Now if we want to use another agent, all we need to do is to create another config file and change its parameters accordingly and then train the agent on this config using \n",
        "the `hive_single_agent_loop`. -->\n",
        "\n",
        "<!-- \n",
        "Now if we want to use another agent, all we need to do is to override the name of the agent field while runnning `hive_single_agent_loop`. Example if we want to replace the DQNAgent by the Rainbow agent, we can do this by executing the following line: -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpCl0jj8VSS5"
      },
      "source": [
        "Moreover, in order to see the results for the ChompLogger(name of logger used by RLHive), you will need to plot your own plots using the log files that are generated in the experiment folder. Below, we provide the code to create this visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGFjHJK6VYNB"
      },
      "outputs": [],
      "source": [
        "# from hive.utils import visualization\n",
        "\n",
        "# visualization.plot_results(experiments_folder='experiment',\n",
        "#     runs_folders=['minigrid_four_rooms_test'],\n",
        "#     x_key='train',\n",
        "#     y_key='train/full_episode_length',rc_params={},\n",
        "#     smoothing_fn=visualization.get_smoothing_fn('exponential', {0.3})\n",
        "#     output_file='minigrid_four_room_test.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a5WQDJj31d6"
      },
      "outputs": [],
      "source": [
        "# visualization.plot_results(experiments_folder='experiment',\n",
        "#     runs_folders=['minigrid_four_rooms_test'],\n",
        "#     run_names=['test 0'],\n",
        "#     x_key='train',\n",
        "#     y_key='train/0_reward',rc_params={},\n",
        "#     smoothing_fn=visualization.get_smoothing_fn('exponential', {0.3})\n",
        "#     output_file='minigrid_reward_test.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that sometimes we have plots with high variance in RL. So they tend to be noisy. For clarity in visualizations, we typically smooth the plots usingg some smoothing functions (as shown in the code above). Here we have used smoothing with exponential function. We recommend using the same for all visualizations in this assignment The code snippets shown above are commented out intentionally. Please DO NOT run these, it is just for demonstration purposes."
      ],
      "metadata": {
        "id": "u06gfgKttam2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT NOTE:** We are providing logs with some information while the training happens. The default frequency is every 100 steps of training but it could be modified by setting the test_frequency to a higher number like 2000 or 10000 (for eg. `test_frequency = 10000`) if you do not want too much information."
      ],
      "metadata": {
        "id": "kJMWrJrMt_PD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6WTu1J7-vUb"
      },
      "source": [
        "# Q1: DQN w/ Varying Buffer Sizes (25 Pts Total)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cLN4u4i_2z3"
      },
      "source": [
        "**Quick Intro to DQN:** The DQN (Deep Q-Network) algorithm was developed by DeepMind in 2015. It was able to solve a wide range of Atari games (some to superhuman level) by combining reinforcement learning and deep neural networks at scale. The algorithm was developed by enhancing a classic RL algorithm called Q-Learning with deep neural networks and a technique called experience replay. It is impractical to represent the Q-function as a table containing values for each combination of state and actions as in Tabular Q-Learning . Instead, we train a function approximator, such as a neural network with parameters , to estimate the Q-values in DQN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0i-jQKaAC4v"
      },
      "source": [
        "A Key component of DQN style algorithms is that, they maintain a buffer of past experiences and sample from this so called 'Replay Buffer' to make updates. In this section of experiments, we will vary the size of this Replay Bufffer and observe the differences in algorithms performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments (15 Pts)\n",
        "\n",
        "As shown in the example section, you will need to fill in the few lines of code below to run these experiments. You will be writing the code to run these experiments for 3 different random seeds and for different values of the parameter of interest.\n",
        "\n",
        "In the section below, we will vary the replay buffer capacity of DQN. Use the DQN config file that was downloaded earlier along with the following parameters as inputs:\n",
        "*   Train steps: 50000\n",
        "*   Replay Buffer Capacity: {100, 10000, 100000}\n",
        "*   Env: 'MiniGrid-Empty-5x5-v0'\n",
        "*   Save results directory: experiment/buffer_size/\n",
        "*   Use exponential smooothing function in the visualization (argument to plot_results)\n",
        "\n",
        "Results should be saved along with the replay buffer capacity values in different folders for ease of visualizing results (For eg. \"experiment/buffer_size/minigrid5*5_100_test/\" where, 100 refers to the capacity of the replay buffer)"
      ],
      "metadata": {
        "id": "iTqSKVkbfmKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT** In order to avoid losing progress if Google Colab runtime disconnects, we will save the generated images to your google drives respectively. So we wil mount the drives to the colab workspace. In order to do that, you need to give access to your google drive storage after running the commands below. Moreover by doing this, we will also make sure you do not have to run the experiments again if your runtime disconnects after one section of experiments finish."
      ],
      "metadata": {
        "id": "z-Ktz8Ns21UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import Image\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "path = \"/content/drive/My Drive/RL_assign4/\"\n",
        "try:\n",
        "  os.mkdir(path)\n",
        "except:\n",
        "  print(\"path exists\")"
      ],
      "metadata": {
        "id": "4fHAqwXVCPPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4PJztg9CMo3"
      },
      "outputs": [],
      "source": [
        "# CODE TO RUN YOUR EXPERIMENTS-----\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qToWaTdMUtZ6"
      },
      "outputs": [],
      "source": [
        "# CODE FOR VISUALIZATIONS (PLOTTING THE EPISODE LENGTH TREND and REWARDS OVER STEPS)------------\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you notice from the examples sectopm, RLHive visualization usually just saves the plots as .png files. Here we will load the saved images from google drive using IPython's Image function to display the plots."
      ],
      "metadata": {
        "id": "TI0seqKVqRxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/My Drive/RL_assign4/buffer_capacity/\"\n",
        "try:\n",
        "  os.mkdir(folder_path)\n",
        "except:\n",
        "  print(\"path exists\")\n",
        "I_episode = cv2.imread('/content/minigrid_episode_length_cap_test.png')\n",
        "I_rewards = cv2.imread('/content/minigrid_reward_cap_test.png')\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/buffer_capacity/minigrid_episode_length_cap_test.png', I_episode)\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/buffer_capacity/minigrid_reward_cap_test.png', I_rewards)\n",
        "Image('/content/drive/My Drive/RL_assign4/buffer_capacity/minigrid_episode_length_cap_test.png')"
      ],
      "metadata": {
        "id": "sjHYhYs320SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('/content/drive/My Drive/RL_assign4/buffer_capacity/minigrid_reward_cap_test.png')"
      ],
      "metadata": {
        "id": "71nzpQO_CAxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions (10 Pts)\n",
        "\n",
        "1. What happens when the buffer size in DQN is at either extremes i.e when the buffer size is very small(100) and when the buffer size is very large(100000)? (6pts)\n"
      ],
      "metadata": {
        "id": "pT7IqJdSfZmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: "
      ],
      "metadata": {
        "id": "xBPwSww09Q_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Do you observe these effects in the plots you get above? If so, explain or else why not? (4pts)"
      ],
      "metadata": {
        "id": "4b9IDbvB9PrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "rM9Hm1-g9cFj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZX-2my0CgFp"
      },
      "source": [
        "# Q2: DQN w/ Varying target network update frequency (30 Pts Total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHipsSNYCgFq"
      },
      "source": [
        "In synchronous learning methods like DQN, there are usually two neural networks. One referred to as an 'Online' network and other referred to as the 'Target' network. The target network is a copy of the online network and it typically is used for one main purpose. Please give your intuition regarding its purpose in DQN below. This parameter copies are updated at a frequency called the target network update frequency. We will also vary this frequency in the experiments below and observe changes in DQN's performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions (15 Pts)\n",
        "\n",
        "1. Why is a Target Network needed in case of DQN? (5Pts)"
      ],
      "metadata": {
        "id": "-UmkNlApraXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "1LEfAOG_-GaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What happens when the target network update frequency is very high i.e you update it quite often like after every update of online network? What would happen if the update frequency is very delayed like 10000? (5Pts)"
      ],
      "metadata": {
        "id": "wfCN0_5U-JLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "xx0boLGv-L0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Do you see similar trends in your plots based on your intuition from the previous question? (5Pts)"
      ],
      "metadata": {
        "id": "-2fO8jpa-Nsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "8ImPpZsP-RGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments (15 Pts)\n"
      ],
      "metadata": {
        "id": "EJD83ursrkis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again as in the previous section, you will need to fill in the few lines of code below to run these experiments. You will be writing the code to run these experiments for 3 different random seeds and for different values of the parameter of interest.\n",
        "\n",
        "In the section below, we will vary the target network update frequency of DQN.\n",
        "In this section of experiments, we will vary target network update frequency and observe the differences in algorithms performance. Here we will consider three different options for the frequency: {1, 100, 1000}\n",
        "\n",
        "Use the DQN config file that was downloaded earlier along with the following parameters as inputs:\n",
        "* Train steps: 100000\n",
        "* Target Network Update frequency: {1, 100, 10000}\n",
        "* Env: 'MiniGrid-Empty-16x16-v0'\n",
        "* Save results directory: experiment/target_freq/\n",
        "* Use exponential smooothing function in the visualization (argument to plot_results)\n",
        "\n",
        "Results should be saved along with the target network update frequency values in different folders for ease of visualizing results. (For eg. \"experiment/target_freq/minigrid16*16_100_test/\" where, 100 refers to the target network update frequency value)"
      ],
      "metadata": {
        "id": "htfg373-rpjU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqIE3IfaCgFs"
      },
      "outputs": [],
      "source": [
        "# CODE TO RUN YOUR EXPERIMENTS-----\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq3VB4kcX2hO"
      },
      "outputs": [],
      "source": [
        "# CODE FOR VISUALIZATIONS (PLOTTING THE EPISODE LENGTH TREND and REWARDS OVER STEPS)------------\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT** In order to avoid losing progress if Google Colab runtime disconnects, we will save the generated images to your google drives respectively. This way, we will make sure you do not have to run the experiments again if your runtime disconnects after one section of experiments finish.\n",
        "\n",
        "If you notice from the examples sectopm, RLHive visualization usually just saves the plots as .png files. Here we will load the saved images from google drive using IPython's Image function to display the plots."
      ],
      "metadata": {
        "id": "dRDK4RZ18VT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import Image\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "sXeExDs_CtWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/My Drive/RL_assign4/target_freq/\"\n",
        "try:\n",
        "  os.mkdir(folder_path)\n",
        "except:\n",
        "  print(\"path exists\")\n",
        "I_episode = cv2.imread('/content/minigrid_episode_length_freq_test.png')\n",
        "I_rewards = cv2.imread('/content/minigrid_reward_freq_test.png')\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/target_freq/minigrid_episode_length_freq_test.png', I_episode)\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/target_freq/minigrid_reward_freq_test.png', I_rewards)\n",
        "Image('/content/drive/My Drive/RL_assign4/target_freq/minigrid_episode_length_freq_test.png')"
      ],
      "metadata": {
        "id": "h4LAEv9U8xXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('/content/drive/My Drive/RL_assign4/target_freq/minigrid_reward_freq_test.png')"
      ],
      "metadata": {
        "id": "XDH9thi9CltY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5lztGdESx9"
      },
      "source": [
        "# Q3: n-step updates in DQN (35 Pts Total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_4RzPqjESx_"
      },
      "source": [
        "Multi-step learning computes the temporal difference error using multi-step transition, instead of a single step. It is usually referred to as the n-step TD update. DQN uses a single-step update by default. In the below experiments we will try to vary this update horizon to larger numbers and observe how that affects performance of a DQN agent."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions (20 Pts)\n",
        "\n",
        "1. What is the significance of having multi-step TD updates in DQN? On an intutitive level, is this helpful for an RL Agent? If so, why? (5Pts)"
      ],
      "metadata": {
        "id": "8-4hjKxltYlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "d0Ff0wPC-2i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are some disadvantages of having a higher update horizon? (5Pts)\n"
      ],
      "metadata": {
        "id": "C_5dAeX_-5KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "U29-WPik_AFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What happens when the update horizon is very high in DQN? Does it help with learning? If so why is that? Or If not please explain your intuitions. (5Pts)"
      ],
      "metadata": {
        "id": "DEBTwcat_Bse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "ueWsBZVV_JSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Based on your intuitions, do you see similar trend from the visualizations of DQN agent with higher update horizon values? (5Pts)"
      ],
      "metadata": {
        "id": "mh6IOBG4_FeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "QNbp2qLz_HvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments (15 Pts)"
      ],
      "metadata": {
        "id": "SZNhbKr5tg5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again as in the previous section, you will need to fill in the few lines of code below to run these experiments. You will be writing the code to run these experiments for 3 different random seeds and for different values of the parameter of interest.\n",
        "\n",
        "In the section below, we will vary the update horizon in TD update equation of DQN.\n",
        "\n",
        "Here we will consider three different options for the update horizon values: {1, 3, 10}\n",
        "\n",
        "Use the DQN config file that was downloaded earlier along with the following parameters as inputs:\n",
        "* Train steps: 50000\n",
        "* Update Horizon values: {1, 3, 10}\n",
        "* Env: 'MiniGrid-Empty-5x5-v0'\n",
        "* Save results directory: experiment/n_step/\n",
        "* Use exponential smooothing function in the visualization (argument to plot_results)\n",
        "\n",
        "Results should be saved along with the target network update frequency values in different folders for ease of visualizing results. (For eg. \"experiment/n_step/minigrid5*5_3_test/\" where, 3 refers to the update horizon value)"
      ],
      "metadata": {
        "id": "XWDtOTYctkcd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnHjk9MpESyB"
      },
      "outputs": [],
      "source": [
        "# CODE TO RUN YOUR EXPERIMENTS-----\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXFKzLX6ZcKQ"
      },
      "outputs": [],
      "source": [
        "# CODE FOR VISUALIZATIONS (PLOTTING THE EPISODE LENGTH TREND and REWARDS OVER STEPS)------------\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT** In order to avoid losing progress if Google Colab runtime disconnects, we will save the generated images to your google drives respectively. This way, we will make sure you do not have to run the experiments again if your runtime disconnects after one section of experiments finish.\n",
        "\n",
        "If you notice from the examples sectopm, RLHive visualization usually just saves the plots as .png files. Here we will load the saved images from google drive using IPython's Image function to display the plots."
      ],
      "metadata": {
        "id": "aJ1DsFdy8haV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import Image\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "sQQBalSgCwnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrQSI1-ojvAF"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/My Drive/RL_assign4/n_step/\"\n",
        "try:\n",
        "  os.mkdir(folder_path)\n",
        "except:\n",
        "  print(\"path exists\")\n",
        "I_episode = cv2.imread('/content/minigrid_episode_length_n_step_test.png')\n",
        "I_rewards = cv2.imread('/content/minigrid_reward_n_step_test.png')\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/n_step/minigrid_episode_length_n_step_test.png', I_episode)\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/n_step/minigrid_reward_n_step_test.png', I_rewards)\n",
        "Image('/content/drive/My Drive/RL_assign4/n_step/minigrid_episode_length_n_step_test.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image('/content/drive/My Drive/RL_assign4/n_step/minigrid_reward_n_step_test.png')"
      ],
      "metadata": {
        "id": "LW1ASKofC3Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq--Dnk6kTFP"
      },
      "source": [
        "# Q4: Ablation Studies (40 Pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHBrKKoih_tt"
      },
      "source": [
        "## Rainbow Ablations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD3XSZIrKOBy"
      },
      "source": [
        "**Quick Intro to Rainbow:** Rainbow is another Value based algorithm like DQN but it is a single agent that integrates several variants of DQN that offfer substantial benefits. Moreover, Rainbow is shown to have better performance than any of the baseline methods included in the ensemble. There are a total of 6 different baseline algorithms included in Rainbow such as:\n",
        "1. DQN (multi-step)\n",
        "2. Double DQN or DDQN\n",
        "3. Prioritized DDQN\n",
        "4. Dueling DDQN\n",
        "5. Distributional DQN\n",
        "6. Noisy DQN\n",
        "\n",
        "Moreover, please note that, in this section we will consider a modified version of Rainbow and only include the algorithms covered in class i.e {DQN, Double DQN or DDQN, Prioritized DDQN, Multi-step DQN}.\n",
        "\n",
        "In this section, we will try to see the contribution of each individual component in Rainbow and try to answer some questions based on this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vA0w88dl9yg"
      },
      "source": [
        "### Ablations to understand the contribution of each baseline in Rainbow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z7-9PDTl9yq"
      },
      "source": [
        "\n",
        "## Questions (20 Pts)\n",
        "1. What are your observations from the following plots? Which baseline contributes most to the performance of Rainbow? and Why? (10 Pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "AktWkC9YAID1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find the original Rainbow Paper and see which component contributes the most to Rainbow's performance. Which factors does it depend on? (10 Pts)"
      ],
      "metadata": {
        "id": "CY_YVzf-AK6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "Aq3OREV8AMvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments (30 Pts)"
      ],
      "metadata": {
        "id": "Xvi02wiW12Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again as in the previous section, you will need to fill in the few lines of code below to run these experiments. You will be writing the code to run these experiments for 3 different random seeds and for different values of the parameter of interest.\n",
        "\n",
        "Now in the following experiment, we will run some ablation studies to understand the impact or contribution of each baseline method in Rainbow. We would like to compare the performance of Rainbow with DQN and 5 other agents. \n",
        "\n",
        "The other agents compared here are:\n",
        "\n",
        "1. Rainbow w/o double DQN\n",
        "2. Rainbow w/o prioritized DDQN\n",
        "3. Rainbow w/o multi-step DQN\n",
        "4. DQN with same config parameters as Rainbow\n",
        "\n",
        "(**NOTE** We are using a modified version of Rainbow based on the concepts covered in class)\n",
        "\n",
        "Use the Rainbow config file that was downloaded earlier along with the following parameters as inputs:\n",
        "* Train steps: 100000\n",
        "* Env: 'MiniGrid-Empty-5x5-v0'\n",
        "* Set the corresponding component of Rainbow to 'False' or 'True' depending on the experiment\n",
        "* Save results directory: experiment/rainbow_ablations/\n",
        "* Use exponential smooothing function in the visualization (argument to plot_results)\n",
        "\n",
        "Note that you should edit the Rainbow config file provided for runnning DQN in this question. (To ensure they have similar config parameters to compare)\n",
        "\n",
        "Results should be saved along with the modified agents name in different folders for ease of visualizing results. (For eg. \"experiment/rainbow_ablations/minigrid5*5_full_rainbow_test/\" where, \"full rainbow\" refers to the modified rainbow agent with the 3 components)"
      ],
      "metadata": {
        "id": "AxN3-gU-vv5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzVAXtzDhEUh"
      },
      "outputs": [],
      "source": [
        "# CODE TO RUN YOUR EXPERIMENTS-----\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUyMLoFDl9yq"
      },
      "outputs": [],
      "source": [
        "# CODE FOR VISUALIZATIONS (PLOTTING THE EPISODE LENGTH TREND and REWARDS OVER STEPS)------------\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# END SOLUTION\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT** In order to avoid losing progress if Google Colab runtime disconnects, we will save the generated images to your google drives respectively. This way, we will make sure you do not have to run the experiments again if your runtime disconnects after one section of experiments finish.\n",
        "\n",
        "If you notice from the examples sectopm, RLHive visualization usually just saves the plots as .png files. Here we will load the saved images from google drive using IPython's Image function to display the plots."
      ],
      "metadata": {
        "id": "6gNpf9GP8oAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import Image\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "2AFgl-lqC9V2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ffbe1c-3cd2-4cd4-abc9-9d345e6d9957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS_LY11hhyiu"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/My Drive/RL_assign4/rainbow_ablations/\"\n",
        "try:\n",
        "  os.mkdir(folder_path)\n",
        "except:\n",
        "  print(\"path exists\")\n",
        "I_episode = cv2.imread('/content/minigrid_episode_length_rainbow_ablations_test.png')\n",
        "I_rewards = cv2.imread('/content/minigrid_reward_rainbow_ablations_test.png')\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/rainbow_ablations/minigrid_episode_length_rainbow_ablations_test.png', I_episode)\n",
        "cv2.imwrite('/content/drive/My Drive/RL_assign4/rainbow_ablations/minigrid_reward_rainbow_ablations_test.png', I_rewards)\n",
        "Image('/content/drive/My Drive/RL_assign4/rainbow_ablations/minigrid_episode_length_rainbow_ablations_test.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image('/content/drive/My Drive/RL_assign4/rainbow_ablations/minigrid_reward_rainbow_ablations_test.png')"
      ],
      "metadata": {
        "id": "9WLs2S9lDE0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/experiment.zip /content/experiment"
      ],
      "metadata": {
        "id": "nv72sX3aiFNK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JHBrKKoih_tt"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}