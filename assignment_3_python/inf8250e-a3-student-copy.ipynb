{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTcTWPbRDXkm"
   },
   "source": [
    "# Assignment 3 -- Policy Gradients\n",
    "## Instructions:\n",
    "* This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
    "* Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
    "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
    "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
    "* If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
    "\n",
    "This assignment has 4 parts. The goals of these parts are:\n",
    "- **Part 1**: Implementing a parameterized (neural network) policy with PyTorch\n",
    "- **Part 2**: Understanding the REINFORCE algorithm\n",
    "- **Part 3**: Extending the REINFORCE algorithm with a baseline\n",
    "- **Part 4**: Understanding Actor-Critic\n",
    "\n",
    "\n",
    "### Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install otter-grader\n",
    "!pip install gym[classic_control]\n",
    "!git clone https://github.com/chandar-lab/INF8250e-assignments-public.git public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import otter\n",
    "grader = otter.Notebook(colab=True, tests_dir='./public/assignment_3_python/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAeKW1awDf9f",
    "outputId": "a2a8eda5-557f-4a2c-d501-0e2ec4b00825"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as torchdist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "\n",
    "from gym.envs.classic_control import CartPoleEnv\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADESCOPE_ENV_VAR = \"RUNNING_IN_GRADESCOPE\"\n",
    "\n",
    "def running_in_gradescope():\n",
    "    var = os.getenv(GRADESCOPE_ENV_VAR)\n",
    "    if var is None:\n",
    "        return False\n",
    "    return var == 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV1l7_Q2EMuy"
   },
   "source": [
    "## The Environment\n",
    "\n",
    "For this assignment, we will use  `CartPole-v0` from OpenAI Gym. In this environment, the goal is to balance an inverted pendulum on a cart by moving the cart laterally. The state of the agent has four components:\n",
    "\n",
    "- The horizontal position of the cart, $x$\n",
    "- The velocity of the cart, $\\dot{x}$\n",
    "- The angle of the pendulum, measured relative to the vertical axis, $\\theta$\n",
    "- The angular velocity of the pendulum, $\\dot\\theta$\n",
    "\n",
    "There are two actions, which apply leftward and rightward force to the cart.\n",
    "\n",
    "The agent receives a reward of $1$ at each timestep,and the episode ends when the pendulum drops too far ($|\\theta|$ gets large, around $\\pi/2$ radians) or when the cart goes out of bounds. Also, the environment terminates after 200 steps if it hasn't already terminated, so the greatest possible return is $200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "raHs7Jj79zw9",
    "outputId": "26914874-572d-44ef-ebd6-1a98e3a5cded"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "cartpole_img = env.render(mode='rgb_array')\n",
    "plt.imshow(cartpole_img)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-Y49oxALyvq"
   },
   "source": [
    "# Part 1. Parameterized Policy (30 pts total)\n",
    "\n",
    "In this assignment, we will be studying Policy Gradient algorithms. In these algorithms, rather than using action-values to select actions, the policy itself is parameterized (in our case, by a neural network), and the policy is optimized directly via gradient ascent.\n",
    "\n",
    "We will use a neural network to represent the policy here. The input to the neural network is a state, and the output should somehow encode a probability distribution over the action space. Our environment has a discrete action space, so the policy should output parameters for a *Categorical distribution*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b-HE0MVAoGc"
   },
   "source": [
    "## 1a: The Policy Network (5 pts)\n",
    "As a first step, fill in the `policy_init_network` function, which should return a torch neural net that will be to produce policy distributions for input states. You are free to experiment with different neural network architectures if you desire, but we recommend the following. Using `torch.nn.Sequential`, make a multilayer perceptron (MLP) with the following layers:\n",
    "\n",
    "1. A linear layer of size `(state space dimension, 32)`, followed by a ReLU activation\n",
    "1. A linear layer of size `(32, 32)`, followed by a ReLU activation\n",
    "1. A linear layer of size `(32, number of actions)` followed by a Softmax activation, to make a probability distribution over actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFc7LOjmJejc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def policy_init_network(env: gym.Env) -> nn.Module:\n",
    "  # ---------------------------------\n",
    "  ...\n",
    "  # ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-esPol7gSH2l"
   },
   "source": [
    "## 1b: The Policy Class (15 pts)\n",
    "In this part, we will build a class to represent the parameterized policy. This will be done in a few steps.\n",
    "\n",
    "### Part I: The constructor\n",
    "In the constructor of the `Policy` class, initialize the variable `opt`, which will be used to optimize the policy parameters. This variable should be a `torch.optim.Optimizer`.\n",
    "\n",
    "### Part II: The policy distribution\n",
    "Fill in the `dist` method. This method takes as input a state and outputs a torch [`Distribution`](https://pytorch.org/docs/stable/distributions.html) over actions.\n",
    "\n",
    "### Part III: Sampling actions\n",
    "Fill in the `action` method. This method samples a random action from the policy at a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmTbUIJ-PYAz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "  def __init__(self,\n",
    "               env: gym.Env,\n",
    "               network: nn.Module,\n",
    "               discount=0.99,\n",
    "               name=\"Abstract Policy\"):\n",
    "    self.name = name\n",
    "    self.network = network\n",
    "    self.discount = discount\n",
    "\n",
    "    self.env = env\n",
    "    self.obs_dim = env.observation_space.shape[0]\n",
    "    self.n_actions = env.action_space.n\n",
    "\n",
    "    # Your code here  (Part I)\n",
    "    # ==========================\n",
    "    ...\n",
    "    # ==========================\n",
    "\n",
    "  \"\"\"\n",
    "  Get the distribution over actions for a given state\n",
    "  \"\"\"\n",
    "  def dist(self, x: np.ndarray) -> torchdist.distribution.Distribution:\n",
    "    dist = None\n",
    "    # Your code here (Part II)\n",
    "    # ========================\n",
    "    ...\n",
    "    # ========================\n",
    "    return dist\n",
    "\n",
    "  \"\"\"\n",
    "  Sample a random action from the policy at a given state\n",
    "\n",
    "  Input: a state encoded as a numpy array\n",
    "  Output: an action encoded as an int\n",
    "  \"\"\"\n",
    "  def action(self, x: np.ndarray) -> int:\n",
    "    action = None\n",
    "    # Your code here (Part III)\n",
    "    # ========================\n",
    "    ...\n",
    "    # ========================\n",
    "    return action\n",
    "\n",
    "  def update(self, states, actions, rewards, dones) -> float:\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c: Generating Rollouts (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdgVCpU8Savd"
   },
   "source": [
    "Now, fill in the following function that rolls out $N$ episodes in the environment with the policy. The function should return `(states, actions, rewards, dones)` where\n",
    "\n",
    "1. `states` is a record of the states observed over the course of the episodes.\n",
    "1. `actions` is a record of the actions taken.\n",
    "1. `rewards` is a record of the rewards earned.\n",
    "1. `dones` is an array of `bool`s that marks where the episodes end. For example, if each episode is 3 steps long, `dones = [False, False, True, False, False, True,...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy1iw7lWSB4H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rollout(env: gym.Env, policy: Policy, N=1, render=False):\n",
    "  states = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  dones = []\n",
    "  # Your code here\n",
    "  #==========================\n",
    "  ...\n",
    "  #==========================\n",
    "  if render:\n",
    "    env.close()\n",
    "  return (np.array(states), np.array(actions), np.array(rewards), np.array(dones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE (47 pts total)\n",
    "In this section, we will implement the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jocPdu0gdDTs"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "## 2a: Discounting Rewards (22 pts total)\n",
    "\n",
    "This problem has 3 parts.\n",
    "\n",
    "Recall the form of the REINFORCE policy gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{G^{\\pi_\\theta}\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
    "$$\n",
    "\n",
    "Here $\\pi_\\theta$ is the parameterized (neural net) policy with parameters $\\theta$, and $G^{\\pi_\\theta}$ is the random variable corresponding to the discounted return induced by following $\\pi_\\theta$. Note that at timestep $k$, action $a_k$ had no influence on rewards incurred before timestep $k$. For this reason, it is generally preferred to compute the following,\n",
    "\n",
    "$$\n",
    "\\widehat\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{G^{\\pi_\\theta}_k\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "G^{\\pi_\\theta}_k = \\sum_{t=k}^T\\gamma^{t-k}r(s_k, a_k)\n",
    "$$\n",
    "\n",
    "### Part I (5 pts):\n",
    "**Question**: Why do you think it is preferred to substitute $\\nabla_\\theta J(\\theta)$ for $\\widehat\\nabla_\\theta J(\\theta)$ in policy gradient algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1CQGAPksPIR"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Part II (5 points):\n",
    "\n",
    "It may help to make the assumption that at the end of an episode, the agent keeps receiving the final reward indefinitely. So, when an episode ends at time $T$, the \"effective reward\" at that last timestep is\n",
    "\n",
    "$$\n",
    "r_T = r(s_T, a_T) + \\gamma r(s_T, a_T) + \\gamma^2r(s_T, a_T) + \\dots\\\\\n",
    "= r(s_T, a_T)\\left(\\sum_{k=0}^\\infty\\gamma^k\\right) \n",
    "$$\n",
    "\n",
    "It's like as if the agent remains in its terminal state forever. When computing the discounted return of a trajectory, you may replace the terminal reward with this \"effective reward\". We will call this \"the perpetuity trick\".\n",
    "\n",
    "**Question**: When should one apply the \"perpetuity trick\", and what effect would you expect it to have on training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sdb2afgwhPbW"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Part III (12 points):\n",
    "Implement the function `discounted_returns`, which computes the values $(G_1^{\\pi_\\theta},G_2^{\\pi_\\theta},\\dots)$ for a given sequence of rewards.\n",
    "\n",
    "The function takes three arguments:\n",
    "\n",
    "1. `rewards`: An array of rewards, which may have been collected over several trajectories. \n",
    "1. `dones`: An array of `bool`s, which mark where trajectories ended.\n",
    "1. `discount`: The discount factor.\n",
    "1. `perpetuity`: A boolean that indicates whether or not the perpetuity trick should be used.\n",
    "\n",
    "The output of the function should be a list of the same length as `rewards` containing the cumulative discounted future returns starting at each step in the reward sequence. Mathematically, for some index $k$, if $T$ is the first index after $k$ for which `dones[T] = True`, then\n",
    "\n",
    "$$\n",
    "\\texttt{returns}[k] = \\texttt{rewards}[k] + \\gamma\\texttt{rewards}[k+1] + \\dots + \\gamma^{T-k}\\texttt{rewards}[T]\\quad \\text{(+ perpetuity, if applicable)}\n",
    "$$\n",
    "\n",
    "For example, suppose we gather data from two trajectories, which had rewards `[1,2,3]` and `[4, 2, 1]` respectively. Then:\n",
    "\n",
    "- `rewards = [1,2,3,4,2,1]`\n",
    "- `dones = [False, False, True, False, False, True]`\n",
    "\n",
    "For `discount = 0.5`, the output should be \n",
    "\n",
    "- Without perpetuity trick: `[2.75, 3.5, 3, 5.25, 2.5, 1]`.\n",
    "- With perpetuity trick: `[3.5, 5, 6, 5.5, 3, 2]`.\n",
    "\n",
    "**NOTE**: The output should be a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hoadj-naiOJk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discounted_returns(rewards: np.ndarray, dones: np.ndarray, discount:float, perpetuity = False) -> np.ndarray:\n",
    "  # Your code here\n",
    "  # ===============================\n",
    "  ...\n",
    "  # ===============================\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2a.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TbcxxeokvSp"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2b: The REINFORCE Update (10 pts)\n",
    "Finally, we'll implement REINFORCE. Fill in the `update` method for the `REINFORCEPolicy` class below. This method takes the following inputs:\n",
    "\n",
    "1. `states`: An array of observed states over the course of $N$ episodes.\n",
    "1. `actions`: An array of actions taken at the corresponding `states`.\n",
    "1. `rewards`: An array of rewards received, where `rewards[k]` is the reward for taking actions `actions[k]` at state `states[k]`.\n",
    "1. `dones`: An array of `bool`s marking the end of episodes.\n",
    "\n",
    "This method should perform the following:\n",
    "- Compute the average policy gradient \"loss\", which is $-\\sum_{n=1}^{T}G_n^{\\pi_\\theta}\\log\\pi_\\theta(a_n\\mid s_n)$,  averaged over all trajectories\n",
    "- Compute the policy gradient\n",
    "- Update the policy parameters\n",
    "\n",
    "The method should return a dictionary that contains information from the update. For now, the dictionary should only have one entry with key `'policy_loss'` that contains a scalar loss from the policy gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHIT-dd_k05l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class REINFORCEPolicy(Policy):\n",
    "  def __init__(self,\n",
    "               env: gym.Env,\n",
    "               network: nn.Module,\n",
    "               discount=0.99,\n",
    "               name=\"Plain REINFORCE\"):\n",
    "    super().__init__(env, network, discount=discount, name=name)\n",
    "    \n",
    "  \"\"\"\n",
    "  Perform a gradient update\n",
    "  Inputs:\n",
    "    states, actions, rewards, dones: Output from rollout method\n",
    "  Returns:\n",
    "    Dictionary with the following keys:\n",
    "    - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken)\n",
    "  \"\"\"  \n",
    "  def update(self, states, actions, rewards, dones) -> float:\n",
    "    loss_dict = {}\n",
    "    # Your code here\n",
    "    # ======================\n",
    "    ...\n",
    "    # ======================\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYI2MRIOyKk9"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2c: Experiments (15 pts)\n",
    "\n",
    "The code below will train agents with REINFORCE using a varying number $N$ of trajectories per gradient estimate, for a fixed number of gradient steps. After each gradient step, we roll out 5 episodes and plot the mean and variance of their returns. Before running the experiments, think about how you expect the performance of the agents to depend on $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "G3BidIGPzqcB",
    "outputId": "c1b73872-a8a5-493d-bedd-18e5e9c79daa"
   },
   "outputs": [],
   "source": [
    "Ns = [1, 5, 10]\n",
    "env = gym.make('CartPole-v0')\n",
    "agents = [\n",
    "    REINFORCEPolicy(env, policy_init_network(env))\n",
    "    for _ in Ns\n",
    "]\n",
    "\n",
    "gradient_steps = 400\n",
    "scores = [np.zeros(gradient_steps) for N in Ns]\n",
    "stds = [np.zeros(gradient_steps) for N in Ns]\n",
    "\n",
    "test_runs = 5\n",
    "\n",
    "def rollout_score(env, policy):\n",
    "  _, _, rewards, _ = rollout(env, policy, N=1)\n",
    "  return np.sum(rewards)\n",
    "\n",
    "gs = list(range(gradient_steps))\n",
    "\n",
    "cmap = plt.get_cmap('viridis')\n",
    "fig, (ret_ax, loss_ax) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "if not running_in_gradescope():\n",
    "    for i in range(len(Ns)):\n",
    "      reinforce_policy = agents[i]\n",
    "      N = Ns[i]\n",
    "      print(f\"N = {N}\")\n",
    "      losses = []\n",
    "      for g in tqdm(range(gradient_steps)):\n",
    "        states, actions, rewards, dones = rollout(env, reinforce_policy, N=N)\n",
    "        loss = reinforce_policy.update(states, actions, rewards, dones)\n",
    "        losses.append(loss['policy_loss'])\n",
    "        res = [rollout_score(env, reinforce_policy) for _ in range(test_runs)]\n",
    "        scores[i][g] = np.mean(res)\n",
    "        stds[i][g] = np.std(res)\n",
    "      color = cmap(i / len(Ns))\n",
    "      ret_ax.plot(gs, scores[i], label=f\"N={N}\", color = color)\n",
    "      ret_ax.fill_between(gs, scores[i] - stds[i], scores[i] + stds[i], alpha=0.3, color=color)\n",
    "      loss_ax.plot(gs, losses, label=f\"N={N}\", color = color)\n",
    "    ret_ax.legend()\n",
    "    ret_ax.grid(True)\n",
    "    ret_ax.margins(0)\n",
    "    ret_ax.set_title('Episode return')\n",
    "    loss_ax.legend()\n",
    "    loss_ax.grid(True)\n",
    "    loss_ax.margins(0)\n",
    "    loss_ax.set_title(\"Policy loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Based on your results, report how the number of rollout trajectories per update affected the learning process. Do your results match your expectations? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFJSefg4ig9k"
   },
   "source": [
    "# 3. REINFORCE with Baseline (40 pts total)\n",
    "When using a baseline in REINFORCE, the policy gradient formula is modified to the following,\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{(G^{\\pi_\\theta} - b(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
    "$$\n",
    "\n",
    "for some function $b:\\mathcal{S}\\to\\mathbf{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3a: Understanding the baseline (12 pts)\n",
    "1. **(3 pts)** What purpose does the baseline serve?\n",
    "1. **(3 pts)** If the baseline is a constant (that is, $b(s_1) = b(s_2)$ for any pair of states $(s_1, s_2)$), should we expect the performance of REINFORCE with this baseline to be any different from standard REINFORCE?\n",
    "1. **(3 pts)** Why can't the baseline be a function of the action as well as the state? \n",
    "1. **(3 pts)** Does the inclusion of an arbitrary baseline always help? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLDPKzevkubU"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 3b: The Value Function (3 pts)\n",
    "In our experiments, we will use the value function as our baseline. It will be necessary to learn the value function from data, so our baseline will have the form\n",
    "\n",
    "$$\n",
    "b(s) = V^{\\pi_\\theta}_\\phi(s)\n",
    "$$\n",
    "\n",
    "where $\\phi$ denotes the parameters of the value function.\n",
    "\n",
    "Fill in the code for the construction of the value function neural net in `value_init_network`. The network architecture should be similar to that of the policy network besides the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMOJYE4rktzZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def value_init_network(env: gym.Env) -> nn.Module:\n",
    "  # Your code here\n",
    "  # ===========================================================\n",
    "  ...\n",
    "  # ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3c: REINFORCE with Baseline (10 pts)\n",
    "\n",
    "Fill in the constructor and the `update` method for `REINFORCEWithBaselinePolicy`.\n",
    "\n",
    "The constructor should do set two variables:\n",
    "* `self.value_network`: the value function neural network\n",
    "* `self.value_opt`: the `torch.optim.Optimizer` for the value function parameters\n",
    "\n",
    "This method should perform the following:\n",
    "- Compute the \"policy gradient loss\", using the value predictions from the value function network instead of the Monte Carlo return estimates\n",
    "- Compute the policy gradient, again using the value predictions from the value function network instead of the Monte Carlo return estimates\n",
    "- Update the policy parameters\n",
    "- Compute the \"value loss\", which is mean squared difference between the Monte Carlo return estimates and the value function network predictions at each state in the trajectory\n",
    "- Update the value function network parameters\n",
    "\n",
    "As with the standard REINFORCE case, the `update` method returns a dictionary with a key `'policy_loss'` reflecting the loss w.r.t. the policy gradient objective. For `REINFORCEWithBaselinePolicy`'s `update` method, however, the dictionary should also have a key `'value_loss'` reflecting the loss w.r.t. the value function error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class REINFORCEWithBaselinePolicy(Policy):\n",
    "  def __init__(self,\n",
    "               env: gym.Env,\n",
    "               policy_network: nn.Module,\n",
    "               value_network: nn.Module,\n",
    "               discount=0.99,\n",
    "               name=\"REINFORCE with Baseline\"):\n",
    "    super().__init__(env, policy_network, discount=discount, name=name)\n",
    "    # Your code here\n",
    "    # ===========================================================\n",
    "    ...\n",
    "    # ===========================================================\n",
    "  \n",
    "  \"\"\"\n",
    "  Perform a gradient update\n",
    "  Inputs:\n",
    "    states, actions, rewards, dones: Output from rollout method\n",
    "  Returns:\n",
    "    Dictionary with the following keys:\n",
    "    - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken)\n",
    "    - \"value_loss\": float of the squared TD error\n",
    "  \"\"\"  \n",
    "  def update(self, states, actions, rewards, dones) -> float:\n",
    "    loss_dict = {}\n",
    "    # Your code here\n",
    "    # ======================\n",
    "    ...\n",
    "    # ======================\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me_n0O1yvdbm"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 3d: Experiments (15 pts)\n",
    "\n",
    "The code below will train agents with REINFORCE with and without the value function baseline. Think about how you expect the return and loss curves to behave with and without the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "0ATEh_JdvUIS",
    "outputId": "68b9fee1-aaa4-4016-c3e0-6bd2387106b4"
   },
   "outputs": [],
   "source": [
    "agents = [\n",
    "    REINFORCEPolicy(env, policy_init_network(env)),\n",
    "    REINFORCEWithBaselinePolicy(env, policy_init_network(env), value_init_network(env))\n",
    "]\n",
    "\n",
    "gradient_steps = 400\n",
    "scores = [np.zeros(gradient_steps) for _ in agents]\n",
    "stds = [np.zeros(gradient_steps) for _ in agents]\n",
    "\n",
    "test_runs = 5\n",
    "\n",
    "def rollout_score(env, policy):\n",
    "  _, _, rewards, _ = rollout(env, policy, N=1)\n",
    "  return np.sum(rewards)\n",
    "\n",
    "gs = list(range(gradient_steps))\n",
    "\n",
    "cmap = plt.get_cmap('viridis')\n",
    "fig, (ret_ax, loss_ax, value_ax) = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "value_losses = []\n",
    "\n",
    "if not running_in_gradescope():\n",
    "    for i in range(len(agents)):\n",
    "      reinforce_policy = agents[i]\n",
    "      print(f\"Training {reinforce_policy.name}\")\n",
    "      losses = []\n",
    "      for g in tqdm(range(gradient_steps)):\n",
    "        states, actions, rewards, dones = rollout(env, reinforce_policy, N=1)\n",
    "        loss = reinforce_policy.update(states, actions, rewards, dones)\n",
    "        losses.append(loss['policy_loss'])\n",
    "        if 'value_loss' in loss.keys():\n",
    "          value_losses.append(loss['value_loss'])\n",
    "        res = [rollout_score(env, reinforce_policy) for _ in range(test_runs)]\n",
    "        scores[i][g] = np.mean(res)\n",
    "        stds[i][g] = np.std(res)\n",
    "      color = cmap(i / len(agents))\n",
    "      ret_ax.plot(gs, scores[i], label=reinforce_policy.name, color = color)\n",
    "      ret_ax.fill_between(gs, scores[i] - stds[i], scores[i] + stds[i], alpha=0.3, color=color)\n",
    "      loss_ax.plot(gs, losses, label=reinforce_policy.name, color = color)\n",
    "    ret_ax.legend()\n",
    "    ret_ax.grid(True)\n",
    "    ret_ax.margins(0)\n",
    "    ret_ax.set_title('Episode return')\n",
    "    loss_ax.legend()\n",
    "    loss_ax.grid(True)\n",
    "    loss_ax.margins(0)\n",
    "    loss_ax.set_title(\"Policy loss\")\n",
    "    value_ax.plot(gs, value_losses, color = color)\n",
    "    value_ax.grid(True)\n",
    "    value_ax.margins(0)\n",
    "    value_ax.set_title(\"Value loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "In your experiments, how did the use of the value function baseline affect your results? Explain the results you observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhRgHXlL4fyH"
   },
   "source": [
    "# 4. Actor-Critic (50 pts total)\n",
    "\n",
    "Finally, we will experiment with an *actor-critic* algorithm. Recall that the gradient rule for REINFORCE with the value function baseline has the following form,\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{(G^{\\pi_\\theta} - V^{\\pi_\\theta}_\\phi(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\n",
    "\\mathbf{E}\\left\\{G^{\\pi_\\theta}\\mid s_0 = s\\right\\} = \\mathbf{E}_{a\\sim\\pi(\\cdot\\mid s),s'\\sim P(\\cdot\\mid s, a)}\\left\\{r(s, a) + \\gamma V^{\\pi_\\theta}(s')\\right\\}\n",
    "$$\n",
    "\n",
    "Because of this, actor-critic algorithms estimate $G^{\\pi_\\theta}$ by $r(s, a) + \\gamma V^{\\pi_\\theta}(s')$. Thus, we can compute one gradient *per environment step*, since we no longer need data from the entire trajectory to estimate $G^{\\pi_\\theta}$. The gradient rule for the policy network (actor) is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J_{\\text{actor}}(\\theta) = \\mathbf{E}\\left\\{(r_k + \\gamma V^{\\pi_\\theta}_\\phi(s_{k+1}) - V^{\\pi_\\theta}_\\phi(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
    "$$\n",
    "\n",
    "for the policy parameters. The value network (critic) is trained to minimize the mean squared TD error:\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi J_{\\text{critic}}(\\phi) = \\frac{1}{2}\\left(V^{\\pi_\\theta}_\\phi(s_k) - \\texttt{stop_gradient}\\left(r_k + \\gamma V^{\\pi_\\theta}_\\phi(s_{k+1})\\right)\\right)^2\n",
    "$$\n",
    "\n",
    "where $\\texttt{stop_gradient}$ enforces that no gradients flow through its argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a: Understanding Actor-Critic (15 pts total)\n",
    "\n",
    "This question is split into three conceptual questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Part I: Bias in Actor-Critic (5 pts)\n",
    "It is said that actor-critic policy gradients are more biased than REINFORCE policy gradients. Explain what this means. Are actor-critic policy gradients more biased than REINFORCE policy gradients computed with the value function baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Part II: Per-Step Updates (5 pts)\n",
    "Even though actor-critic algorithms can perform one update per step, the gradients are computed based on data from only one state transition as opposed to REINFORCE gradients which are averaged over $T$ state transitions. What is the benefit of updating once per environment step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Part III: Lifelong Learning (5 pts)\n",
    "Imagine a scenario where an RL agent is to be deployed on a strange planet that we do not know how to simulate. Once we drop the robot on this planet, we can never interact with it again: it just autonomously learns from environment interactions for the rest of its life. Would you prefer to employ Actor-Critic or REINFORCE with baseline for this problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBYhN3uU9CDk"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 4b: Implementing Actor-Critic (20 pts)\n",
    "Fill out the `ActorCriticPolicy` class below, according to the guidelines in the code. The `policy_init_network` and `value_init_network` methods will be used to instantiate the neural nets for the actor-critic, however they are trained differently in the actor-critic algorithm.\n",
    "\n",
    "Rather than implementing an `update` method for actor-critic, we will implement a method `train_episode` which rolls out an episode, performing updates at each step. More precisely, `train_episode` should do the following:\n",
    "\n",
    "1. Reset the environment to a starting state\n",
    "1. For each environment step:\n",
    "    1. Choose an action\n",
    "    1. Perform an environment step with the chosen action, observing the next state, reward, and terminal signal\n",
    "    1. Update both the actor and critic networks based on this transition\n",
    "1. Return a dictionary with the same entries as `REINFORCEWithBaselinePolicy`'s `update` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObWy7-gv9BRf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActorCriticPolicy(Policy):\n",
    "  def __init__(self,\n",
    "               env: gym.Env,\n",
    "               policy_network: nn.Module,\n",
    "               value_network: nn.Module,\n",
    "               discount=0.99,\n",
    "               name=\"Actor-Critic\"):\n",
    "    super().__init__(env, policy_network, discount=discount, name=name)\n",
    "    # Your code here\n",
    "    # Initialize self.value_network and self.value_opt like before\n",
    "    # ===========================================================\n",
    "    ...\n",
    "    # ===========================================================\n",
    "  \n",
    "  \"\"\"\n",
    "  Run a training episode\n",
    "  Inputs:\n",
    "    states, actions, rewards, dones: Output from rollout method\n",
    "  Returns:\n",
    "    Dictionary with the following keys:\n",
    "    - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken),\n",
    "                     averaged over the episode\n",
    "    - \"value_loss\": float of the squared TD error averaged over the episode\n",
    "  \"\"\"  \n",
    "  def train_episode(self) -> float:\n",
    "    loss_dict = {}\n",
    "    # Your code here\n",
    "    # ======================\n",
    "    ...\n",
    "    # ======================\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"question 4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U41kVqmBKHC"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 4c: Experiments (15 pts)\n",
    "\n",
    "In the following experiments, we test the following agents:\n",
    "\n",
    "- REINFORCE with one trajectory per gradient update\n",
    "- REINFORCE with the value function baseline, one trajectory per gradient update\n",
    "- Actor-Critic\n",
    "\n",
    "Each agent is trained for 400 episodes, and the experiment is repeated 6 times with different random seeds. The plot displays the mean and variance of the return across the seeds for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "SEEDS = [4, 8, 15, 16, 23, 42]\n",
    "\n",
    "episodes = 400\n",
    "eval_runs = 5\n",
    "eval_every = 5\n",
    "epochs = list(range(0, episodes, eval_every))\n",
    "\n",
    "cmap = plt.get_cmap('viridis')\n",
    "plt.grid(True)\n",
    "plt.margins(0)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "\n",
    "pg_constructor = lambda: REINFORCEPolicy(env, policy_init_network(env))\n",
    "pg_baseline_constructor = lambda: REINFORCEWithBaselinePolicy(env, policy_init_network(env), value_init_network(env))\n",
    "\n",
    "ac_agent_constructor = lambda: ActorCriticPolicy(env, policy_init_network(env), value_init_network(env))\n",
    "\n",
    "if not running_in_gradescope():\n",
    "    ### ACTOR CRITIC\n",
    "    ac_agents = {}\n",
    "    ac_score_traces = {}\n",
    "    print(f\"Training Actor-Critic\")\n",
    "    for (seed, ep) in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n",
    "        if seed not in ac_agents.keys():\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            ac_agents[seed] = ac_agent_constructor()\n",
    "            ac_score_traces[seed] = []\n",
    "            ac_agents[seed].env.seed(seed)\n",
    "        agent = ac_agents[seed]\n",
    "        loss = agent.train_episode()\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n",
    "            ac_score_traces[seed].append(np.mean(res))\n",
    "\n",
    "    ac_data = np.vstack([ac_score_traces[seed] for seed in SEEDS])\n",
    "    ac_score_mean = np.mean(ac_data, axis=0)\n",
    "    ac_score_std = np.std(ac_data, axis=0)\n",
    "\n",
    "    plt.plot(epochs, ac_score_mean, color=cmap(0.8), label='Actor Critic')\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        ac_score_mean - ac_score_std,\n",
    "        ac_score_mean + ac_score_std,\n",
    "        color=cmap(0.8),\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    ### REINFORCE WITH BASELINE\n",
    "    pg_baseline_agents = {}\n",
    "    pg_baseline_score_traces = {}\n",
    "    print(f\"Training REINFORCE with Baseline\")\n",
    "    for (seed, ep) in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n",
    "        if seed not in pg_baseline_agents.keys():\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            pg_baseline_agents[seed] = pg_baseline_constructor()\n",
    "            pg_baseline_score_traces[seed] = []\n",
    "            env.seed(seed)\n",
    "        agent = pg_baseline_agents[seed]\n",
    "        states, actions, rewards, dones = rollout(env, agent, N=1)\n",
    "        loss = agent.update(states, actions, rewards, dones)\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n",
    "            pg_baseline_score_traces[seed].append(np.mean(res))\n",
    "\n",
    "    pg_baseline_data = np.vstack([pg_baseline_score_traces[seed] for seed in SEEDS])\n",
    "    pg_baseline_score_mean = np.mean(pg_baseline_data, axis=0)\n",
    "    pg_baseline_score_std = np.std(pg_baseline_data, axis=0)\n",
    "\n",
    "    plt.plot(epochs, pg_baseline_score_mean, color=cmap(0.5), label='REINFORCE with Baseline')\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        pg_baseline_score_mean - pg_baseline_score_std,\n",
    "        pg_baseline_score_mean + pg_baseline_score_std,\n",
    "        color=cmap(0.5),\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    ### REINFORCE\n",
    "    pg_agents = {}\n",
    "    pg_score_traces = {}\n",
    "    print(f\"Training REINFORCE with Baseline\")\n",
    "    for (seed, ep) in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n",
    "        if seed not in pg_agents.keys():\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            pg_agents[seed] = pg_constructor()\n",
    "            pg_score_traces[seed] = []\n",
    "            env.seed(seed)\n",
    "        agent = pg_agents[seed]\n",
    "        states, actions, rewards, dones = rollout(env, agent, N=1)\n",
    "        loss = agent.update(states, actions, rewards, dones)\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n",
    "            pg_score_traces[seed].append(np.mean(res))\n",
    "\n",
    "    pg_data = np.vstack([pg_score_traces[seed] for seed in SEEDS])\n",
    "    pg_score_mean = np.mean(pg_data, axis=0)\n",
    "    pg_score_std = np.std(pg_data, axis=0)\n",
    "\n",
    "    plt.plot(epochs, pg_score_mean, color=cmap(0.2), label='REINFORCE')\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        pg_score_mean - pg_score_std,\n",
    "        pg_score_mean + pg_score_std,\n",
    "        color=cmap(0.2),\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Based on your experiments, does actor-critic perform favorably to REINFORCE (with and/or without baseline)? Explain your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "question 1a": {
     "name": "question 1a",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_env = gym.make('CartPole-v0')\n>>> test_output = policy_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_out = test_output(test_state)\n>>> np.testing.assert_allclose(test_out.shape[0], 2)\n>>> np.testing.assert_allclose(test_out.detach().numpy().sum(), 1.0, rtol=1e-5, atol=0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1b": {
     "name": "question 1b",
     "points": [
      5,
      5
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_state = env.reset()\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal(isinstance(test_policy.opt, torch.optim.Optimizer), True)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_state = env.reset()\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> assert_equal(isinstance(test_policy.dist(test_state).param_shape, torch.Size), True)\n>>> assert_equal(test_policy.dist(test_state).param_shape, torch.Size([1,2]))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 1c": {
     "name": "question 1c",
     "points": [
      3,
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> test_len_rollout = test_s.shape[0]\n>>> assert_equal(isinstance(test_s, np.ndarray), True)\n>>> assert_equal(isinstance(test_a, np.ndarray), True)\n>>> assert_equal(isinstance(test_r, np.ndarray), True)\n>>> assert_equal(isinstance(test_d, np.ndarray), True)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> test_len_rollout = test_s.shape[0]\n>>> assert_equal(test_s.shape, (test_len_rollout, 4))\n>>> assert_equal(test_a.shape, (test_len_rollout,))\n>>> assert_equal(test_r.shape, (test_len_rollout,))\n>>> assert_equal(test_d.shape, (test_len_rollout,))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2a.3": {
     "name": "question 2a.3",
     "points": [
      2,
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_len_rollout = 5\n>>> test_d = np.array([True for _ in range(test_len_rollout)])\n>>> test_r = np.ones(test_len_rollout)\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_ret = discounted_returns(test_r, test_d, 0.99)\n>>> assert_equal(test_ret.shape[0], test_len_rollout)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> test_rewards = [1.0,2.0,3.0,4.0,2.0,1.0]\n>>> test_dones = [False, False, True, False, False, True]\n>>> test_return = discounted_returns(test_rewards, test_dones, 0.5)\n>>> actual_return = np.array([2.75, 3.5, 3.0, 5.25, 2.5, 1.0])\n>>> assert_equal(test_return, actual_return)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> test_rewards = [1.0,2.0,3.0,4.0,2.0,1.0]\n>>> test_dones = [False, False, True, False, False, True]\n>>> test_return_perpetuity = discounted_returns(test_rewards, test_dones, 0.5, perpetuity=True)\n>>> actual_return_perpetuity = np.array([3.5, 5., 6., 5.5, 3., 2.])\n>>> assert_equal(test_return_perpetuity, actual_return_perpetuity)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 2b": {
     "name": "question 2b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_env = gym.make('CartPole-v0')\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_reinforce = REINFORCEPolicy(test_env, policy_init_network(test_env), 0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce, 2)\n>>> loss = test_reinforce.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n>>> assert_equal(isinstance(loss['policy_loss'], float), True)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 3b": {
     "name": "question 3b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_env = gym.make('CartPole-v0')\n>>> test_state = test_env.reset()\n>>> test_value_net = value_init_network(test_env)\n>>> test_value = test_value_net(torch.FloatTensor(test_state))\n>>> np.testing.assert_allclose(test_value.shape[0], 1)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 3c": {
     "name": "question 3c",
     "points": [
      1,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> assert_equal(isinstance(test_reinforce_baseline.value_opt, torch.optim.Optimizer), True)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce_baseline, 2)\n>>> loss = test_reinforce_baseline.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce_baseline, 2)\n>>> loss = test_reinforce_baseline.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal('value_loss' in loss, True)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "question 4b": {
     "name": "question 4b",
     "points": [
      2,
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_actor_critic = ActorCriticPolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> loss = test_actor_critic.train_episode()\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_actor_critic = ActorCriticPolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> loss = test_actor_critic.train_episode()\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n>>> assert_equal('value_loss' in loss, True)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
